{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 5.1 Sigmoid Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from typing import Callable, Type\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Training loop function\n",
    "from IPython.display import clear_output, display\n",
    "import time\n",
    "\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions \n",
    "\n",
    "- to visualize\n",
    "- to focus on stuff that really matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     11,
     17
    ]
   },
   "outputs": [],
   "source": [
    "def viz_1d(X, Y, title = None):\n",
    "    \n",
    "    if isinstance(Y, torch.Tensor):\n",
    "        Y = Y.detach()\n",
    "        \n",
    "    plt.figure(figsize=(7, 4))\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.scatter(X, Y)\n",
    "    plt.show()\n",
    "    \n",
    "def viz_2d(X, Y, title=None):\n",
    "    plt.figure(figsize=(6,6), dpi=150)\n",
    "    plt.title(\"Dataset\" if not title else title)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=\"viridis\", s=10)\n",
    "    plt.show()\n",
    "  \n",
    "def viz_all(X, Y, model, f=None, ax=None):\n",
    "    sns.set(style=\"white\")\n",
    "\n",
    "    minx, maxx = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "    miny, maxy = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "    # xx, yy = np.mgrid[minx:maxx:.01, miny:maxy:.01]\n",
    "    print(minx, maxx, miny, maxy)\n",
    "    xx, yy = np.mgrid[minx.item():maxx.item():.05, miny.item():maxy.item():.05]\n",
    "    # xx, yy = np.mgrid[-1.1:1.1:.01, -1.1:1.1:.01]\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    batch = torch.from_numpy(grid).type(torch.float32)\n",
    "    with torch.no_grad():\n",
    "        probs = model(batch).reshape(xx.shape)\n",
    "        probs = probs.numpy().reshape(xx.shape)\n",
    "        \n",
    "    if (f is None and ax is not None) or (f is not None and ax is None):\n",
    "        raise ValueError(f\"F and AX both should either be None or not\")\n",
    "    \n",
    "    old_ax = False\n",
    "    if f is None and ax is None:\n",
    "        f, ax = plt.subplots(figsize=(16, 10))\n",
    "        old_ax = True\n",
    "        \n",
    "        \n",
    "    ax.set_title(\"Decision boundary\", fontsize=14)\n",
    "    contour = ax.contourf(xx, yy, probs, 25, cmap=\"RdBu\",\n",
    "                          vmin=-0.1, vmax=1.1)\n",
    "    ax_c = f.colorbar(contour)\n",
    "    ax_c.set_label(\"$P(y = 1)$\")\n",
    "    ax_c.set_ticks([0, .25, .5, .75, 1])\n",
    "\n",
    "    ax.scatter(X[:,0], X[:, 1], c=Y[:], s=50,\n",
    "               cmap=\"RdBu\", vmin=-.2, vmax=1.2,\n",
    "               edgecolor=\"white\", linewidth=1)\n",
    "\n",
    "    ax.set(xlabel=\"$X_1$\", ylabel=\"$X_2$\")\n",
    "    \n",
    "    if not old_ax:\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "def report(model: torch.nn.Module, title='Parameters before update:'):\n",
    "    op = title[::]\n",
    "    for np in model.named_parameters():\n",
    "        op += f\"\\n\\t{np[0]}: {np[1].data.item() if not np[1].shape.__len__()>1 else np[1].data}\\tgrad: {np[1].grad if np[1].grad is not None else None}\"\n",
    "    print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ze train function\n",
    "%matplotlib inline\n",
    "def train(\n",
    "        model: torch.nn.Module,\n",
    "          X: torch.Tensor, \n",
    "          Y: torch.Tensor,\n",
    "          loss_function: Callable = torch.nn.MSELoss(), \n",
    "          optimizer_class: Type = torch.optim.SGD,\n",
    "          epochs: int = 200,\n",
    "          learning_rate: float = 0.01,\n",
    "          sleep_time: int | None = None,\n",
    "          early_stopping_threshold: float = 0.002,\n",
    "          viz_every: int = 1\n",
    "          ) -> torch.nn.Module:\n",
    "    \n",
    "    optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs+1):\n",
    "\n",
    "        # Do a train step\n",
    "        # # Everything else is just 'fluff'\n",
    "        optimizer.zero_grad()\n",
    "        Y_pred = model(X)\n",
    "        loss = loss_function(Y_pred, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % viz_every == 0:\n",
    "            clear_output(wait=True)\n",
    "            fig, ax = plt.subplots(figsize=(6, 6), dpi=100)\n",
    "\n",
    "            # Scatter your training data\n",
    "            plt.scatter(X[:,0], X[:,1], c=Y, alpha=0.8)\n",
    "\n",
    "            # Generate a grid and get predictions (as before)\n",
    "            x_min, x_max = X[:,0].min() - 0.5, X[:,0].max() + 0.5\n",
    "            y_min, y_max = X[:,1].min() - 0.5, X[:,1].max() + 0.5\n",
    "            xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                                np.linspace(y_min, y_max, 100))\n",
    "            grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "            with torch.no_grad():\n",
    "                logits = model(torch.from_numpy(grid_points).float())\n",
    "                # Single-logit model => use sigmoid\n",
    "                probs = torch.sigmoid(logits).numpy().reshape(xx.shape)\n",
    "\n",
    "            # 1) Show filled contours of probability. \n",
    "            contour_f = plt.contourf(xx, yy, probs, levels=20, alpha=0.6)\n",
    "            plt.colorbar(contour_f, label=\"Predicted Probability (Class 1)\")\n",
    "\n",
    "            # 2) Optionally plot the 0.5 boundary in black\n",
    "            plt.contour(xx, yy, probs, levels=[0.5], colors='black')\n",
    "\n",
    "            plt.title(f\"Epoch: {epoch} - Loss: {loss.item()}\")\n",
    "            plt.show()\n",
    "            # viz_all(X=X, Y=Y, model=model, f=fig, ax=ax)\n",
    "\n",
    "\n",
    "        # Early stopping\n",
    "        if loss.item() < early_stopping_threshold:\n",
    "            print(f\"Converged at epoch {epoch}\")\n",
    "            break\n",
    "        \n",
    "        if sleep_time is not None:\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "        # Log results\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # Visualize loss\n",
    "    plt.figure(figsize=(5,3), dpi=200)\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Training Loss (last_avg: {float(np.mean(losses[-len(losses)//10:])):.6f})')\n",
    "    plt.xticks()\n",
    "    plt.yticks()\n",
    "    plt.show()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation constants\n",
    "n_samples = 2_000\n",
    "noise_scale = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A circle\n",
    "radius = 1.0 \n",
    "\n",
    "def gen_circle(X: torch.Tensor, noise_scale: float) -> torch.Tensor:\n",
    "    # distance from origin\n",
    "    distances = torch.sqrt(X[:, 0]**2 + X[:, 1]**2)\n",
    "\n",
    "    # Add some noise to the distances\n",
    "    distances += torch.randn_like(distances) * noise_scale\n",
    "\n",
    "    # Label: inside or outside a set radius\n",
    "    return (distances < radius).float().unsqueeze(1)  # shape: (300, 1)\n",
    "    \n",
    "\n",
    "# Generating random x \n",
    "X_cir = 4 * (torch.rand(n_samples, 2) - 0.5)  # shape: (n, 2), in [-2,2] square\n",
    "\n",
    "# Lets try and visualize just the X\n",
    "viz_2d(X_cir, torch.ones(X_cir.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets now generate the labels. For each dot we get either a positive class (1) or a negative class (0)\n",
    "Y_cir = gen_circle(X_cir, noise_scale*0.5)\n",
    "\n",
    "viz_2d(X_cir, Y_cir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets use our MLP as defined in prev session (just varying the input dimension also)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train(..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at what the model actually does\n",
    "i = 10\n",
    "_x = X_cir[:i].unsqueeze(0)\n",
    "_y = Y_cir[:i].unsqueeze(1)\n",
    "\n",
    "print(_x, _y)\n",
    "print(_x.shape, _y.shape)\n",
    "\n",
    "_ypred = MLP(2)(_x)\n",
    "_ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# But we want an output between 0 and 1\n",
    "\n",
    "lets treat model output as a probability distribution b/w 0  and 1. Treat the output as a confidence.as_integer_ratio\n",
    "\n",
    "# But how do we restrict output to this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.linspace(-5, 10, n_samples).unsqueeze(1)\n",
    "viz_1d(X, ...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets incorporate this into our model?\n",
    "# Lets use our MLP as defined in prev session (just varying the input dimension also)\n",
    "class MLPClf(torch.nn.Module):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Its missing one more thing: loss function\n",
    "model = train("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So that's classification\n",
    "\n",
    "We treat the output as a probability distribution between 0.0 (negative class) and 1.0 (positive class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
