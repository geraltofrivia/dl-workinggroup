{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TjTU0Z_waFz"
   },
   "source": [
    "# This notebook has been adapted from \n",
    "\n",
    "Machine Learning for NLP - ENSAE 2022 - Lecture 2 illustration \n",
    "\n",
    "This notebook aims at illustrating the concepts introduced in the [lecture 2](https://nlp-ensae.github.io/files/2-ML-FOR-NLP-2022.pdf) of the Machine Learning for NLP course\n",
    "\n",
    "Slides from https://nlp-ensae.github.io/files/2-ML-FOR-NLP-2022.pdf\n",
    "(Skip to slide 17)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T09:01:22.683857Z",
     "start_time": "2023-02-14T09:01:22.622184Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T08:46:41.870687Z",
     "start_time": "2023-02-09T08:46:41.863976Z"
    }
   },
   "source": [
    "# One Hot Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T09:01:23.019000Z",
     "start_time": "2023-02-14T09:01:23.013747Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab  = {'potato', 'petrolium', 'carrots', 'garlic', 'mac', 'basil', 'piano', 'flour'}\n",
    "vecs = np.zeros((len(vocab), len(vocab)))\n",
    "vecs[[range(len(vocab))], [range(len(vocab))]] = 1\n",
    "\n",
    "potato, petrolium, carrots, garlic, mac, basil, piano, flour = vecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T09:01:23.166021Z",
     "start_time": "2023-02-14T09:01:23.161866Z"
    }
   },
   "outputs": [],
   "source": [
    "# What is the similarity between petrolium and potato?\n",
    "# What about carrots and potato?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity between vectors ðŸ¤”\n",
    "\n",
    "## Dot Product?\n",
    "\n",
    "$$\n",
    "\\textbf{a} = [a_1, a_2, \\dots , a_n]\\\\\\textbf{b} = [b_1, b_2, \\dots , b_n] \\\\ \\textbf{a}.\\textbf{b} = \\sum^{n}_{i=1}a_ib_i\n",
    "$$\n",
    "\n",
    "- Vectors with a larger magnitude will have higher 'score'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine?\n",
    "\n",
    "$$\n",
    "\\text{cos}(\\textbf{a},\\textbf{b}) = \\frac{\\textbf{a} . \\textbf{b}}{||\\textbf{a}|| . ||\\textbf{b}||}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T09:01:23.914610Z",
     "start_time": "2023-02-14T09:01:23.891359Z"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine(v1, v2):\n",
    "  \"\"\"\n",
    "  cosine similarity of two vectors\n",
    "  NB: Standard metric to measure similarity between word vectors\n",
    "  \n",
    "  Hint: look at the imports in cell above ;)\n",
    "  \"\"\"\n",
    "  return dot(v1, v2)/(norm(v1)*norm(v2))\n",
    "\n",
    "a,b = np.array([1,1,1,1]), np.array([-1,2,-1,2])\n",
    "print(f\"Cosine of {a}, {b} = {cosine(a,b)}\")\n",
    "\n",
    "a,b = np.array([1,1,1,1]), np.array([-2,-2,-2,-2])\n",
    "print(f\"Cosine of {a}, {b} = {cosine(a,b)}\")\n",
    "\n",
    "a,b = np.array([1,1,1,1]), np.array([20,20,20,20])\n",
    "print(f\"Cosine of {a}, {b} = {cosine(a,b)}\")\n",
    "\n",
    "a,b = np.array([1,1,1,1]), np.array([-9,20,-7,-4])\n",
    "print(f\"Cosine of {a}, {b} = {cosine(a,b)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems with One-hot Encoding\n",
    "\n",
    "1. Sparsity, parameters are not used properly.\n",
    "2. For one word, only one column of linear transformation matrix is used.\n",
    "    1. Need to learn lexical, semantic and all other notions independently for each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DocumentClassification.jpg](./../resources/imgs/onehotaintshit.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJBKyC-e9Awv"
   },
   "source": [
    "# Count-Based Representations\n",
    "\n",
    "Same dimensions as one-hot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T09:01:24.211839Z",
     "start_time": "2023-02-14T09:01:24.206632Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Co-Occurences of dog, lion and car \n",
    "#  (leash,walk, run, owner, pet, barked, the)\n",
    "dog =  [3,   5,   2,     5,   3,      2,  9]\n",
    "lion = [0,   3,   2,     0,   1,      0,  5]\n",
    "car =  [0,   0,   1,     3,   0,      0,  9]\n",
    "\n",
    "# dog = np.array(dog)\n",
    "# lion = np.array(lion)\n",
    "# car =  np.array(car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T09:01:24.443059Z",
     "start_time": "2023-02-14T09:01:24.437795Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(f'Example 1: Original similarities \\nCosine Similarity dog vs. lion {cosine(dog, lion):.4f} ')\n",
    "print(f'Cosine Similarity dog vs. car {cosine(dog, car):.4f} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T09:01:24.586930Z",
     "start_time": "2023-02-14T09:01:24.580630Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now assume 'the dog' and 'the car' appear much more \n",
    "# (leash, walk, run, owner, pet, barked, the)\n",
    "dog =  [3,   5,   2,     5,   3,      2,  9+25]\n",
    "car =  [0,   0,   1,     3,   0,      0,  9+25]\n",
    "\n",
    "print(f'Example 2: Now assume \"the dog\" and \"the car\" appear much more  \\nCosine Similarity dog vs. lion {cosine(dog, lion):.4f}')\n",
    "print(f'Cosine Similarity dog vs. car {cosine(dog, car):.4f} ')\n",
    "print(f'What do you observe? \\n')\n",
    "# Frequent words impacts a lot the representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T09:01:24.786497Z",
     "start_time": "2023-02-14T09:01:24.781764Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now assume 'the dog' and 'the car' appear much more \n",
    "# (leash,   walk, run, owner, pet, barked, the)\n",
    "dog =  [3+1,   5,   2,     5,   3,      2,   9]\n",
    "car =  [0+1,   0,   1,     3,   0,      0,   9]\n",
    "\n",
    "print(f'Example 3: Now assume \"dog leash\" and \"car leash\" occur.  \\nCosine Similarity dog vs. lion {cosine(dog, lion):.4f}')\n",
    "print(f'Cosine Similarity dog vs. car {cosine(dog, car):.4f} ')\n",
    "print(f'What do you observe? \\n')\n",
    "# Very high sensitivity to rare words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "- Count based similarity is super sensitive to less informational, very frequent word (the)\n",
    "- Count based similarity is super sensitive to rare words as well (leash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T17:14:26.183946Z",
     "start_time": "2023-02-08T17:14:26.174756Z"
    }
   },
   "source": [
    "## Alternatives?\n",
    "\n",
    "TF-IDF solves this problem. \n",
    "\n",
    "[Good Article explaining it](https://towardsdatascience.com/tf-idf-simplified-aba19d5f5530) | \n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "\n",
    "Q: **why not use tf-idf then?**\n",
    "\n",
    "A: Its not a way to represent tokens, only a way to compute a score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pointwise Mutual Information\n",
    "\n",
    "**Estimate association between events** (events = tokens, for us).\n",
    "- `PMI(\"barack\", \"obama\") =   20.0`: Does \"Barack\" often appear with \"Obama\" (yes â†’ high positive value)? **they're correlated**\n",
    "- `PMI(\"barack\", \"pizza\") =    0.3`: Or does it appear together with \"Peacock\" (independent â†’ close to zero)? **they're not correlated**\n",
    "- `PMI(\"barack\", \"racist\") = -13.8`: Or does it appear together with \"racist\"?\n",
    "    - In fact no, it appears less frequently with \"Barack\". I.e., **they're inversely correlated**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [PMI] is one of the most important concepts in NLP - Ch 6, Speech and Language Processing, Jurafsky and Martin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###### Let's work with this co-occurance matrix\n",
    "\n",
    "![coccurance.png](./../resources/imgs/word-wordcooccurancematrix.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T09:01:25.837270Z",
     "start_time": "2023-02-14T09:01:25.820552Z"
    }
   },
   "outputs": [],
   "source": [
    "cooccurance = np.asarray([\n",
    "    [3,  5,   2,   5,     3,   2 ,    8],\n",
    "    [0,  3,   2,   0,     1,   0,     6],\n",
    "    [0,  0,   1,   3,     0,   0,     3]\n",
    "])\n",
    "\n",
    "# (leash, walk, run, owner, pet, barked, the)\n",
    "dog =  np.array([3,  5,   2,   5,     3,   2 ,    8])\n",
    "lion = np.array([0,  3,   2,   0,     1,   0,     6])\n",
    "car =  np.array([0,  0,   1,   3,     0,   0,     3])\n",
    "\n",
    "leash =  np.array([3, 0, 0])\n",
    "walk =   np.array([5, 3, 0])\n",
    "run =    np.array([2, 2, 1])\n",
    "owner =  np.array([5, 0, 3])\n",
    "pet =    np.array([3, 1, 0])\n",
    "barked = np.array([2, 0, 0])\n",
    "the =    np.array([8, 6, 3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PMI Formula\n",
    "    \n",
    "$$\\text{pmi}(x, y) = \\text{ln} \\big( \\frac{p(x, y)}{p(x) p(y)}\\big)$$\n",
    "\n",
    "Its a simple co-relation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T09:01:26.170429Z",
     "start_time": "2023-02-14T09:01:26.165414Z"
    }
   },
   "outputs": [],
   "source": [
    "def ppmi(mat, ix, iy):\n",
    "    pxy = mat[ix, iy]/mat.sum()\n",
    "    px = mat[ix].sum()/mat.sum()\n",
    "    py = mat[:,iy].sum()/mat.sum()\n",
    "    \n",
    "    if pxy == 0 or px == 0 or py == 0:\n",
    "        return 0\n",
    "    \n",
    "    return np.log(pxy/(px*py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T09:01:26.450826Z",
     "start_time": "2023-02-14T09:01:26.443199Z"
    }
   },
   "outputs": [],
   "source": [
    "print(ppmi(cooccurance, 1, 0)) # dog, leash\n",
    "print(ppmi(cooccurance, 0, 1)) # dog, walk\n",
    "print(ppmi(cooccurance, 0, 2)) # dog, run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PMIVEC.jpg](./../resources/imgs/pmivec.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T09:01:27.130532Z",
     "start_time": "2023-02-14T09:01:27.122914Z"
    }
   },
   "outputs": [],
   "source": [
    "def pmivec(cooccurance, ix):\n",
    "    return np.array([ppmi(cooccurance, ix, iy) for iy in range(cooccurance.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T09:01:27.439365Z",
     "start_time": "2023-02-14T09:01:27.429660Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7UQTwR-LODEf",
    "outputId": "bfe4da9f-9a93-4c50-e4a1-3d5b2c8f65cc"
   },
   "outputs": [],
   "source": [
    "dog_pmi = pmivec(cooccurance, 0)\n",
    "lion_pmi = pmivec(cooccurance, 1)\n",
    "car_pmi = pmivec(cooccurance, 2)\n",
    "\n",
    "print(dog_pmi)\n",
    "print(lion_pmi)\n",
    "print(car_pmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T19:03:09.478999Z",
     "start_time": "2023-02-08T19:03:09.469226Z"
    }
   },
   "source": [
    "# Problems with all of these methods?\n",
    "\n",
    "## Dimensionality\n",
    "\n",
    "![NwordsStuff.jpg](./../resources/imgs/nwords_per_doc.png)\n",
    "\n",
    "x axis - number of documents, y axis - number of unique words\n",
    "\n",
    "With more documents, come more words. Every token will be 33k dimensions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJKDdZ96P54f"
   },
   "source": [
    "# Prediction-Based Representation: Word Embeddings\n",
    "\n",
    "Let's move to notebook 6.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Reading\n",
    "\n",
    "- Ch 6, Speech and Language Processing - https://web.stanford.edu/~jurafsky/slp3/6.pdf (great book; great chapter)\n",
    "- Great article on understanding the intuition of PMI - https://svn.spraakdata.gu.se/repos/gerlof/pub/www/Docs/npmi-pfd.pdf"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
