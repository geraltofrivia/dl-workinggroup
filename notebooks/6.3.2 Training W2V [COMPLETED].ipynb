{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align='right'><h3> Yes, we **used similarity** to <font color='red'>find <i>related<i> words</font> but it was designed to <font color='blue'>predict context words</font> ! <h3>  </div>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T21:28:12.505529Z",
     "start_time": "2023-02-15T21:28:12.493148Z"
    }
   },
   "source": [
    "# Word2Vec Core Idea\n",
    "\n",
    "Use similarity of vectors for **word** $w_i$ and **context word** $w_{i+j}$ to calculate the probability of the context given the word, i.e. $p(w_{i+j}|w_i)$.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"../resources/imgs/stanfordw2va.pdf.png\" width=\"80%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T21:48:36.144547Z",
     "start_time": "2023-02-15T21:48:36.131457Z"
    }
   },
   "source": [
    "# Word2Vec Algorithm\n",
    "\n",
    "1. **Data**:\n",
    "    Get A Corpus. Get the vocab\n",
    "2. **Init Vectors**: Each word has a fixed-length vector (say 300 dimensions).\n",
    "3. **Assign Vectors**: n your corpus, collect every word $w$, and the context around it $c$.\n",
    "4. **Probability is Vector Similarity**: Use similarity of vectors for $w$ and $c$ to calculate the probability of the context given the word, i.e. $p(c|w)$.\n",
    "5. **Change Vectors**: Keep adjusting the vectors to maximize this probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now go step-by-step to emulate this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T21:56:40.431758Z",
     "start_time": "2023-02-15T21:56:40.425265Z"
    }
   },
   "source": [
    "# Data\n",
    "\n",
    "## 1. Get a corpus\n",
    "\n",
    "Original model was trained on a 100 billion words part of Google News Corpus. \n",
    "I don't think I can find it, and we are for sure not going to be able to use it.\n",
    "\n",
    "We'll stick with Wikitext 2 version 1\n",
    "\n",
    "Links: [Wikitext 2 Description](https://paperswithcode.com/dataset/wikitext-2) [Wikitext 2 Datasets Page](https://huggingface.co/datasets/wikitext/viewer/wikitext-2-v1/train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T10:30:14.945113Z",
     "start_time": "2023-02-23T10:30:14.768343Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "from tqdm.auto import tqdm, trange\n",
    "from datasets import load_dataset\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "! free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T09:30:52.577095Z",
     "start_time": "2023-02-23T09:30:50.273059Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "wikitext = load_dataset(\"wikitext\", \"wikitext-2-v1\")\n",
    "wikitext, wikitext['train'][10]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T22:01:04.545167Z",
     "start_time": "2023-02-15T22:01:04.536325Z"
    }
   },
   "source": [
    "## Get a Vocab\n",
    "\n",
    "You know the drill by now. Get a counter. Set n_words. Select top-n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T09:30:53.949503Z",
     "start_time": "2023-02-23T09:30:52.578943Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make a vocab\n",
    "word_counter = Counter()\n",
    "for line in tqdm(wikitext['train'], miniters=1000):\n",
    "    word_counter.update(line['text'].split())\n",
    "    \n",
    "len(word_counter), word_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T10:19:49.513998Z",
     "start_time": "2023-02-23T10:19:48.578445Z"
    }
   },
   "outputs": [],
   "source": [
    "unk_token = '<unk>'\n",
    "vocab = {tok: i for i, (tok, freq) in enumerate(word_counter.most_common())}\n",
    "n_words = len(vocab)\n",
    "train_text = [doc['text'].split() for doc in wikitext['train']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Init Vectors\n",
    "\n",
    "Each word gets two vectors. One for when it appears as the center word ($w_i$), and another when it appears as the context word ($w_j$).\n",
    "\n",
    "So we need two matrices: $U \\subset \\mathcal{R}^{300 \\times n}$ and $V \\subset \\mathcal{R}^{300 \\times n}$.\n",
    "\n",
    "\n",
    "PS: why two? Mathematical reasons. One is possible, but not very nice to deal with. Can talk more about if you want. Feel free to ask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T09:30:54.101169Z",
     "start_time": "2023-02-23T09:30:53.972423Z"
    }
   },
   "outputs": [],
   "source": [
    "U = torch.randn(n_words, 300)\n",
    "V = torch.randn(n_words, 300)\n",
    "\n",
    "U.shape, V.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T07:27:07.789977Z",
     "start_time": "2023-02-16T07:27:07.784661Z"
    }
   },
   "source": [
    "# 3. Assign Vectors\n",
    "\n",
    "1. Lets collect words manually\n",
    "2. Lets assign vectors to them\n",
    "3. Lets make an iterator for A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Lets collect words manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T09:30:54.108363Z",
     "start_time": "2023-02-23T09:30:54.102759Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence = \"The history of all existing society is the history of class struggles\".split()\n",
    "pairs = [] # order: (contextword, centerword)\n",
    "for i, centerword in enumerate(sentence):\n",
    "    if i - 1 >= 0:\n",
    "        pairs.append((sentence[i-1], centerword))\n",
    "    if i - 2 >= 0:\n",
    "        pairs.append((sentence[i-2], centerword))\n",
    "    if i + 1 < len(sentence):\n",
    "        pairs.append((sentence[i+1], centerword))\n",
    "    if i + 2 < len(sentence):\n",
    "        pairs.append((sentence[i+2], centerword))\n",
    "pairs.__len__(), pairs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lets assign vectors to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T09:30:54.112790Z",
     "start_time": "2023-02-23T09:30:54.109943Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert words to word ids\n",
    "pairs_wordid = [(vocab[pair[0]], vocab[pair[1]]) for pair in pairs if pair[0] in vocab and pair[1] in vocab]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T09:30:54.117203Z",
     "start_time": "2023-02-23T09:30:54.114401Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lets just focus on one pair for now\n",
    "print(f\"Center: {pairs[38][1]}, Context: {pairs[38][0]}\")\n",
    "pair = pairs_wordid[38]\n",
    "w_v, w_u = pair\n",
    "vec_v = V[w_v]\n",
    "vec_u = U[w_u]\n",
    "\n",
    "\n",
    "print(pair, vec_v.shape, vec_u.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T08:34:01.636507Z",
     "start_time": "2023-02-16T08:34:01.630913Z"
    }
   },
   "source": [
    "### 3. Data Iter, skip for now ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T09:30:54.121325Z",
     "start_time": "2023-02-23T09:30:54.119479Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Probability of Word Pair\n",
    "\n",
    "\n",
    "# W2V Idea: Vector Similarity $\\propto$ Probability\n",
    "\n",
    "<br>\n",
    "<div align=\"center\"> That is, </div>\n",
    "\n",
    "$$\n",
    "p(\\text{problems}|\\text{into}) \\propto \\mathbf{u}_{\\text{into}}^T \\mathbf{v}_{\\text{problems}}\n",
    "$$\n",
    "\n",
    "This is valid for all the words in the vocabulary. So,\n",
    "\n",
    "<div>\n",
    "<img src=\"../resources/imgs/Illustrations-15a.jpg\" width=\"70%\"/>\n",
    "</div>\n",
    "<!-- ![image.png](../resources/imgs/Illustrations-15a.jpg) -->\n",
    "\n",
    "There's a <b><font color=\"maroon\">problem</font></b> however: **Dot product can be negative.**\n",
    "\n",
    "### Solution: $e^x$\n",
    "\n",
    "Exponent is a always postivie function. Additionally, it differentiates very nicely ;)\n",
    "\n",
    "$$\\frac{d}{dx}(e^x) = e^x$$\n",
    "\n",
    "<div>\n",
    "<img src=\"../resources/imgs/exponent.png\" width=\"50%\"/>\n",
    "</div>\n",
    "\n",
    "So we can just take the exponent of the dot product:\n",
    "\n",
    "<div>\n",
    "<img src=\"../resources/imgs/Illustrations-15b.jpg\" width=\"70%\"/>\n",
    "</div>\n",
    "\n",
    "Its easy to get the actual probability from this point onwards. Normalize over all values!\n",
    "\n",
    "<div>\n",
    "<img src=\"../resources/imgs/Illustrations-15c.jpg\" width=\"70%\"/>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "So at the end, we have a way to calculate the probability of \"problems\" being in the context of \"into\".\n",
    "\n",
    "$$ p(w_j|w_i) = \\frac{\\text{exp}(u_i^T v_j)}{\\sum^n_{k=1}\\ \\text{exp}(u_i^T v_k)}$$\n",
    "\n",
    "Or\n",
    "\n",
    "$$ p(w_j|w_i) = \\text{softmax}(u.v_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T09:30:54.127628Z",
     "start_time": "2023-02-23T09:30:54.122402Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.dot(vec_u, vec_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T09:30:54.132298Z",
     "start_time": "2023-02-23T09:30:54.128578Z"
    }
   },
   "outputs": [],
   "source": [
    "# Coming back to code, we have a pair of words:\n",
    "print(f\"Center: {pairs[38][1]}, Context: {pairs[38][0]}\")\n",
    "pair = pairs_wordid[38]\n",
    "w_v, w_u = pair\n",
    "vec_v = V[w_v]\n",
    "vec_u = U[w_u]\n",
    "\n",
    "torch.dot(vec_u, vec_v), vec_v.shape, vec_u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T09:30:54.144745Z",
     "start_time": "2023-02-23T09:30:54.133182Z"
    }
   },
   "outputs": [],
   "source": [
    "# But we need to do this for every word in the vocab. How? Hint: torch.mm \n",
    "torch.mm(V, vec_u.view(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T09:30:54.153332Z",
     "start_time": "2023-02-23T09:30:54.146127Z"
    }
   },
   "outputs": [],
   "source": [
    "# So now we have all the similarities\n",
    "# Lets do the softmax\n",
    "torch.softmax(torch.mm(V, vec_u.view(-1,1)), dim=0).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T21:35:50.395726Z",
     "start_time": "2023-02-15T21:35:50.389506Z"
    }
   },
   "source": [
    "# 4. Probability of Data: Generlization of above\n",
    "<!-- ![math](https://media.tenor.com/DmflrPpBB8cAAAAd/math-calculating.gif \"mathtime\") -->\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"../resources/imgs/stanfordw2va.pdf.png\" width=\"80%\"/>\n",
    "</div>\n",
    "\n",
    "### Probability of seeing this context\n",
    "\n",
    "$$\n",
    "P(\\text{data}_{\\text{local}}) = P(\\text{problems}\\ |\\ \\text{into}) \\times  P(\\text{turning}\\ |\\ \\text{into}) \\times P(\\text{banking}\\ |\\ \\text{into}) \\times  P(\\text{crises}\\ |\\ \\text{into}) \n",
    "$$\n",
    "\n",
    "### Probability of generalizing this over the corpus\n",
    "\n",
    "$$P(\\text{data}) = \\prod_{t=1}^{T} P(\\text{data}_{\\text{local}, t})$$\n",
    "\n",
    "### Likelihood: \n",
    "\n",
    "> Definition: <b> Probability of parameters -> Probability of seeing the data given these parameters </b>\n",
    "\n",
    "\n",
    "\n",
    "$$L(\\theta) = \\prod_{i=1}^{T} \\prod_{-m \\leq j \\leq m, j \\neq 0} P(w_{t+j}|w_t; \\theta)$$\n",
    "\n",
    "where\n",
    "\n",
    "$$ p(w_j|w_i) = \\frac{\\text{exp}(u_i^T v_j)}{\\sum^n_{k=1}\\ \\text{exp}(u_i^T v_k)}$$\n",
    "\n",
    "Or\n",
    "\n",
    "$$ p(w_j|w_i) = \\text{softmax}(u.v_i)$$\n",
    "\n",
    "### Objective: Maximize Likelihood or Minimize Negative Log Likelihood\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{T} \\prod_{i=1}^{T} \\prod_{-m \\leq j \\leq m, j \\neq 0} \\text{log} P(w_{t+j}|w_t; \\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "\n",
    "\n",
    "# So let's start backproping???\n",
    "\n",
    "![computetime](https://media.tenor.com/rDKZFPwK-00AAAAC/the-matrix-keanu-reeves.gif \"backprop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Problems! </font>: Inefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T10:39:05.645894Z",
     "start_time": "2023-02-16T10:39:05.635275Z"
    }
   },
   "source": [
    "For each word pair, we compute a distribution over the enitre vocabulary.\n",
    "Why? To normalize the scores.\n",
    "\n",
    "###### Recall: \n",
    "\n",
    "**score**: $f(u.v)$ or $(u^T v)$. Our $f(.)$ was $\\text{exp}(.)$\n",
    "\n",
    "\n",
    "**normalization**: $\\sum_{i=0}^{|\\text{vocab}|} f(u.v_i)$ <- **nicht gut!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative Sampling\n",
    "\n",
    "> Instead of maximizing the likelihood of data i.e. $P(\\text{data};\\theta)$, we make sure that real data is scored higher than fake data i.e. $f(\\text{data};\\theta) \\gt f(\\neg\\ \\text{data}; \\theta)$.\n",
    "\n",
    "That is, for each word, don't <font color=\"red\">select the right word</font>, but ensure that correct word is scored higher than incorrect word. *Don't need to normalize this way.*\n",
    "\n",
    "**Updated Objective Function**:\n",
    "\n",
    "$$\n",
    "J(\\theta) = - \\text{log}\\ \\sigma (u_{t}^T v_{c}) - \\sum_{k\\in \\text{Samples}} \\text{log}\\ \\sigma ( - u_k^T v_c)\n",
    "$$\n",
    "\n",
    "- For each positive sample, we take `k` negative samples.\n",
    "\n",
    "- Negative words are not uniformly sampled, but with $P(w) = U(w)^{\\frac{3}{4}} / Z$ to counteract Zipf's law.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T09:30:54.156921Z",
     "start_time": "2023-02-23T09:30:54.154798Z"
    }
   },
   "outputs": [],
   "source": [
    "### Let's emulate the model init here\n",
    "\n",
    "# Init the embeddings\n",
    "\n",
    "\n",
    "# Custom Init the embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's emulate the model forward here\n",
    "u, pos, neg = torch.randint(0, len(vocab), (20)), torch.randint(0, len(vocab), (20)), torch.randint(0, len(vocab), (20, 4))\n",
    "\n",
    "# Pos Score\n",
    "\n",
    "# Neg Score\n",
    "\n",
    "# Return Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T10:19:11.877522Z",
     "start_time": "2023-02-23T10:19:11.863283Z"
    }
   },
   "outputs": [],
   "source": [
    "class SkipGramWordEmbeddings(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, vocab_size, emb_dimension):\n",
    "    super().__init__()\n",
    "    self.vocab_size = vocab_size\n",
    "    self.emb_dimension = emb_dimension\n",
    "    self.U = torch.nn.Embedding(vocab_size, emb_dimension)\n",
    "    self.V = torch.nn.Embedding(vocab_size, emb_dimension)\n",
    "\n",
    "    initrange = 1.0 / self.emb_dimension\n",
    "    torch.nn.init.uniform_(self.U.weight.data, -initrange, initrange)\n",
    "    self.V.weight.data.uniform_(-initrange, initrange)\n",
    "#     torch.nn.init.constant_(self.V.weight.data, 0)\n",
    "\n",
    "  def forward(self, u, pos, neg):\n",
    "    vec_u = self.U(u)  # (bs, 300)\n",
    "    vec_pos_v = self.V(pos) # (bs, 300)\n",
    "    vec_neg_v = self.V(neg) # (bs, 300)\n",
    "\n",
    "    score = torch.mul(vec_u, vec_pos_v)\n",
    "    score = torch.sum(score, dim=1)\n",
    "    score = - F.logsigmoid(score)\n",
    "\n",
    "    neg_score = torch.bmm(vec_neg_v, vec_u.unsqueeze(2)).squeeze()\n",
    "    neg_score = torch.sum(neg_score, dim=1)\n",
    "    neg_score = - F.logsigmoid(-1*neg_score).squeeze()\n",
    "    \n",
    "    loss = score + neg_score\n",
    "\n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T09:56:13.128948Z",
     "start_time": "2023-02-23T09:56:13.103988Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class W2VIter:\n",
    "\n",
    "    def __init__(self, vocab, corpus, negatives=4, batchsize=64):\n",
    "        \"\"\"\n",
    "            vocab: dict: key is token, value is id\n",
    "            corpus: List of [ List of tokens ]\n",
    "            batchsize: int\n",
    "        \"\"\"\n",
    "\n",
    "        # Count Word Frequency\n",
    "        wfreq = Counter()\n",
    "        for doc in corpus:\n",
    "            wfreq.update(doc)\n",
    "\n",
    "        # Shuffle the corpus\n",
    "        npr = np.random.permutation(len(corpus))\n",
    "        corpus = [corpus[i] for i in npr]\n",
    "\n",
    "        self._batchsize = batchsize - batchsize % (\n",
    "                    negatives + 1)  # rounded off to negatives+1. E.g. if bs 63, and neg=4; bs = 60\n",
    "        self._negatives = negatives\n",
    "        self._vocab = vocab\n",
    "        self._wordfreq = {tok: wfreq[tok] for tok, _ in vocab.items()}\n",
    "        self._unkid = self._vocab[unk_token]\n",
    "\n",
    "        # Convert corpus to wordids\n",
    "        corpus = [self.get_word_ids(doc) for doc in corpus]\n",
    "\n",
    "        # Convert every document to word pairs (shuffled)\n",
    "        wordpairs = [self.get_word_pairs(doc) for doc in tqdm(corpus) if doc]\n",
    "\n",
    "        self.data = [x for x in wordpairs if x]\n",
    "\n",
    "    def get_word_ids(self, doc):\n",
    "        return [self._vocab.get(tok, self._unkid) for tok in doc]\n",
    "\n",
    "    def get_word_pairs(self, doc):\n",
    "        pairs = []\n",
    "        for i, token in enumerate(doc):\n",
    "            if i - 1 >= 0 and doc[i - 1] != self._unkid:\n",
    "                pairs.append((doc[i - 1], token))\n",
    "            if i - 2 >= 0 and doc[i - 2] != self._unkid:\n",
    "                pairs.append((doc[i - 2], token))\n",
    "            if i + 1 < len(doc) and doc[i + 1] != self._unkid:\n",
    "                pairs.append((doc[i + 1], token))\n",
    "            if i + 2 < len(doc) and doc[i + 2] != self._unkid:\n",
    "                pairs.append((doc[i + 2], token))\n",
    "\n",
    "        # Shuffle the pairs\n",
    "        npr = np.random.permutation(len(pairs))\n",
    "        pairs = [pairs[i] for i in npr]\n",
    "        return pairs\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.docid, self.wordid = 0, 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        bs = int(self._batchsize / (self._negatives + 1))\n",
    "        batch_pos = []\n",
    "\n",
    "        while True:\n",
    "\n",
    "            # If we have already gone through all the documents\n",
    "            if self.docid == len(self.data):\n",
    "                raise StopIteration  # the loop stops,the epoch is over\n",
    "\n",
    "            # get next document\n",
    "            document = self.data[self.docid]\n",
    "\n",
    "            # upto: either batchsize, or doc length whichever is shorter (if doc has 100 pairs, take 60) (if batchsize is 60)\n",
    "            _from = self.wordid\n",
    "            _upto = _from + int(min(bs, len(document) - _from))\n",
    "            batch_pos += document[_from:_upto]\n",
    "\n",
    "            # What to do with global pointers\n",
    "            if _upto >= len(document):\n",
    "                # Lets move to the next document\n",
    "                self.docid += 1\n",
    "                self.wordid = 0\n",
    "            else:\n",
    "                # Still in the same document\n",
    "                self.wordid = _upto\n",
    "\n",
    "            # If the batch is over i.e. we got as many pairs as we wanted, we break this while loop\n",
    "            if len(batch_pos) == int(self._batchsize / (self._negatives + 1)):\n",
    "                break\n",
    "            # If not, we still continue taking pairs from the next document\n",
    "            else:\n",
    "                bs -= (_upto - _from)\n",
    "                \n",
    "        batch_pos = torch.tensor(batch_pos)\n",
    "        u = batch_pos[:,0]\n",
    "        v_pos = batch_pos[:,1]\n",
    "        \n",
    "        # Negatives: for one positive there would be multiple negatives\n",
    "        v_neg = torch.randint(0, len(self._vocab), (v_pos.shape[0], self._negatives))\n",
    "\n",
    "        return u, v_pos, v_neg  # warning they have different shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T10:19:16.325501Z",
     "start_time": "2023-02-23T10:19:16.097642Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Lets try it all out \n",
    "dataiter = W2VIter(vocab, corpus=train_text, negatives=4, batchsize=1000)\n",
    "for batch in dataiter:\n",
    "    break\n",
    "\n",
    "u, pos, neg = batch\n",
    "print(u.shape, pos.shape, neg.shape)\n",
    "\n",
    "model = SkipGramWordEmbeddings(len(vocab), 300)\n",
    "model(u, pos, neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we have time, let's implement the iterator as well?b\n",
    "# If not, let's implement a way to sample words with the custom frequency (PS: torch.distributions <3)\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T10:25:35.764850Z",
     "start_time": "2023-02-23T10:25:31.117856Z"
    }
   },
   "outputs": [],
   "source": [
    "# Okay I guess its time to trainnnn\n",
    "model = SkipGramWordEmbeddings(len(vocab), 300)\n",
    "dataiter = W2VIter(vocab, corpus=train_text, negatives=4, batchsize=1000)\n",
    "\n",
    "epochs = 10\n",
    "lr = 0.2\n",
    "opt = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T10:31:12.404298Z",
     "start_time": "2023-02-23T10:31:12.228945Z"
    }
   },
   "outputs": [],
   "source": [
    "! free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T10:46:24.370929Z",
     "start_time": "2023-02-23T10:31:13.712971Z"
    }
   },
   "outputs": [],
   "source": [
    "# takes half hour on CPU for one epoch x)b\n",
    "\n",
    "model.train()\n",
    "\n",
    "per_epoch_loss = []\n",
    "for e in trange(epochs):\n",
    "    \n",
    "    per_batch_loss = []\n",
    "    for u, pos, neg in tqdm(dataiter):\n",
    "        \n",
    "        # reset gradients\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        loss = model(u, pos, neg)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        per_batch_loss.append(loss.item())\n",
    "    \n",
    "    per_epoch_loss.append(per_batch_loss)\n",
    "    print(f\"{e:4d}: Loss = {np.mean(per_batch_loss)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![image.png](../resources/imgs/trainingw2v.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Reading\n",
    "\n",
    "A great overview of this entire thing - [Blogpost](https://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "\n",
    "\n",
    "Another implementation of the entire thing - [Github](https://github.com/lukysummer/SkipGram_with_NegativeSampling_Pytorch/blob/master/SkipGram_NegativeSampling.ipynb)\n",
    "\n",
    "Skip-Gram embeddings with negative embeddings is implicit factorization of the co-occurance matrix - [Paper](https://papers.nips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf)\n",
    "\n",
    "On Biases in Word Embeddings, and ways to counteract them (ony gender bias targeted in this paper) - [Paper](https://arxiv.org/pdf/1607.06520.pdf)\n",
    "\n",
    "WEAT Test - [Paper](https://arxiv.org/pdf/1608.07187.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further To-dos\n",
    "\n",
    "## 1. Set up a development environment on Grid5000\n",
    "## 2. Implement Model saving, Model Loading\n",
    "## 3. Implement some proxy task to evaluate these embeddings\n",
    "## -1. Replace all of this by Gensim implementation :P"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
