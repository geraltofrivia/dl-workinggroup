{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "X -> Y makes supervised machine learning. We tried with random numbers. We tried with images. Pixels are numbers. Everything is numbers.\n",
    "\n",
    "We can treat text the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:30:52.261004Z",
     "start_time": "2023-01-18T12:30:50.663733Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:30:53.645065Z",
     "start_time": "2023-01-18T12:30:52.262473Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/priyansh/Dev/perm/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25000,\n",
       " {'label': 1,\n",
       "  'text': 'Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the dataset\n",
    "imdb = load_dataset(\"imdb\")\n",
    "\n",
    "len(imdb['train']), imdb['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Vectors\n",
    "\n",
    "![One Hot](https://miro.medium.com/max/828/1*9ZuDXoc2ek-GfHE2esty5A.webp)\n",
    "src - https://medium.com/intelligentmachines/word-embedding-and-one-hot-encoding-ad17b4bbe111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:30:53.650869Z",
     "start_time": "2023-01-18T12:30:53.647058Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'girl', 'called', 'siyana', 'had', 'a', 'little', 'lamb']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = \"A girl called Siyana had a little lamb\".lower()\n",
    "tokens = document.split(' ')\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:30:53.656528Z",
     "start_time": "2023-01-18T12:30:53.652700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0, 'girl': 1, 'called': 2, 'siyana': 3, 'had': 4, 'little': 5, 'lamb': 6}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {}\n",
    "for token in tokens:\n",
    "    token = token.lower()\n",
    "    if not token in vocab:\n",
    "        vocab[token] = len(vocab)\n",
    "        \n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:30:53.661726Z",
     "start_time": "2023-01-18T12:30:53.657685Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0, 'girl': 1, 'called': 2, 'siyana': 3, 'had': 4, 'little': 5, 'lamb': 6}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {}\n",
    "for token in tokens:\n",
    "    vocab.setdefault(token, len(vocab))\n",
    "        \n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:30:53.668976Z",
     "start_time": "2023-01-18T12:30:53.663427Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hots = []\n",
    "one_hots = np.zeros((len(vocab), len(tokens)))\n",
    "for word_nr, token in enumerate(tokens):\n",
    "    word_id = vocab[token]\n",
    "    one_hots[word_id, word_nr] = 1\n",
    "    \n",
    "one_hots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T10:18:20.080338Z",
     "start_time": "2023-01-18T10:18:20.075071Z"
    }
   },
   "source": [
    "## Multi-Hot Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:41:08.713569Z",
     "start_time": "2023-01-18T12:41:07.963074Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lets just work with 1000 documents for now\n",
    "\n",
    "train_text = [instance['text'] for instance in imdb['train']]#[:1000]\n",
    "train_labels = [instance['label'] for instance in imdb['train']]#[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess (estimate: 30-40 minutes).\n",
    "\n",
    "This is the most difficult part ^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenize Text\n",
    "\n",
    "Document is one long string of text -> One unit (pixel) can be a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:41:09.518419Z",
     "start_time": "2023-01-18T12:41:09.508124Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(document):\n",
    "    \"\"\"\n",
    "        1. lowercase everything \n",
    "    \"\"\"\n",
    "    document = document.replace(\".\", \" .\").replace(\"!\", \" !\")\n",
    "    return document.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:41:09.831954Z",
     "start_time": "2023-01-18T12:41:09.826428Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bromwell|High|is|a|cartoon|comedy|.|It|ran|at|the|same|time|as|some|other|programs|about|school|life,|such|as|\"Teachers\"|.|My|35|years|in|the|teaching|profession|lead|me|to|believe|that|Bromwell|High\\'s|satire|is|much|closer|to|reality|than|is|\"Teachers\"|.|The|scramble|to|survive|financially,|the|insightful|students|who|can|see|right|through|their|pathetic|teachers\\'|pomp,|the|pettiness|of|the|whole|situation,|all|remind|me|of|the|schools|I|knew|and|their|students|.|When|I|saw|the|episode|in|which|a|student|repeatedly|tried|to|burn|down|the|school,|I|immediately|recalled|.|.|.|.|.|.|.|.|.|at|.|.|.|.|.|.|.|.|.|.|High|.|A|classic|line:|INSPECTOR:|I\\'m|here|to|sack|one|of|your|teachers|.|STUDENT:|Welcome|to|Bromwell|High|.|I|expect|that|many|adults|of|my|age|think|that|Bromwell|High|is|far|fetched|.|What|a|pity|that|it|isn\\'t|!'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test our basic tokenizer\n",
    "'|'.join(tokenize(imdb['train'][0]['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Tokenizer\n",
    "This one is actually useful.\n",
    "\n",
    "### To Install It\n",
    "\n",
    "`! pip install spacy`\n",
    "\n",
    "`! python -m spacy download en_core_web_sm`\n",
    "\n",
    "within jupyter cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:41:10.663247Z",
     "start_time": "2023-01-18T12:41:10.182161Z"
    }
   },
   "outputs": [],
   "source": [
    "# Actually useful tokenizer\n",
    "import spacy\n",
    "exclude = [\"parser\", \"tagger\", \"ner\", \"textcat\", \"attribute_ruler\", \"lemmatizer\"]\n",
    "nlp = spacy.load(\"en_core_web_sm\", exclude=exclude)\n",
    "\n",
    "def get_spacy_tokens(text):\n",
    "    return [token.text for token in nlp(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:41:10.677249Z",
     "start_time": "2023-01-18T12:41:10.664693Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bromwell|High|is|a|cartoon|comedy|.|It|ran|at|the|same|time|as|some|other|programs|about|school|life|,|such|as|\"|Teachers|\"|.|My|35|years|in|the|teaching|profession|lead|me|to|believe|that|Bromwell|High|\\'s|satire|is|much|closer|to|reality|than|is|\"|Teachers|\"|.|The|scramble|to|survive|financially|,|the|insightful|students|who|can|see|right|through|their|pathetic|teachers|\\'|pomp|,|the|pettiness|of|the|whole|situation|,|all|remind|me|of|the|schools|I|knew|and|their|students|.|When|I|saw|the|episode|in|which|a|student|repeatedly|tried|to|burn|down|the|school|,|I|immediately|recalled|.........|at|..........|High|.|A|classic|line|:|INSPECTOR|:|I|\\'m|here|to|sack|one|of|your|teachers|.|STUDENT|:|Welcome|to|Bromwell|High|.|I|expect|that|many|adults|of|my|age|think|that|Bromwell|High|is|far|fetched|.|What|a|pity|that|it|is|n\\'t|!'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Spacy Tokenizer\n",
    "tokens = get_spacy_tokens(train_text[0])\n",
    "'|'.join(tokens)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T21:55:28.747463Z",
     "start_time": "2023-01-17T21:55:25.159926Z"
    }
   },
   "source": [
    "# Tokenize Everything \n",
    "# NOTE: this might take some time. There are ways to speed it up but we dont need that for now\n",
    "tokenized_train_text = []\n",
    "for text in tqdm(train_text):\n",
    "   tokenized_train_text.append(get_spacy_tokens(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:53.826901Z",
     "start_time": "2023-01-18T12:41:10.724969Z"
    }
   },
   "outputs": [],
   "source": [
    "# This takes 2-5 minutes. We'll talk till then ^^'\n",
    "\n",
    "train_docs = list(nlp.pipe(train_text))\n",
    "tokenized_train_text = [[tok.text for tok in doc] for doc in train_docs]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T09:47:52.045634Z",
     "start_time": "2023-01-18T09:47:52.040144Z"
    }
   },
   "source": [
    "## 1.1 Exploring the data \n",
    "\n",
    "- Length Distribution\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T09:54:58.791484Z",
     "start_time": "2023-01-18T09:54:58.573048Z"
    }
   },
   "source": [
    "lens = [len(doc) for doc in tokenized_train_text]\n",
    "bin_ranges = [i for i in range(0, max(lens), max(lens)//50)]\n",
    "\n",
    "#create histogram with 4 bins\n",
    "print(f\"Over {len(lens)} documents, the mean is {np.mean(lens):.2f} ± {np.std(lens):.2f}\")\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.hist(lens, bins=bin_ranges, edgecolor='black')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T09:55:22.669868Z",
     "start_time": "2023-01-18T09:55:22.664095Z"
    }
   },
   "source": [
    "# Lets decide on a maximum length of documents based on this. Say 200\n",
    "maxlen = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:55.594901Z",
     "start_time": "2023-01-18T12:43:53.828333Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a2b0295b174b7b9c3e168d3304b35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "121064"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The same setdefault stuff we did above\n",
    "vocab = {}\n",
    "for document in tqdm(tokenized_train_text):\n",
    "    for token in document:\n",
    "        vocab.setdefault(token, len(vocab))\n",
    "    \n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T22:07:52.341213Z",
     "start_time": "2023-01-17T22:07:52.334930Z"
    }
   },
   "source": [
    "### That's way too many words. 121064?\n",
    "\n",
    "Let's make sure we have only 10000 words. First 10000 words?\n",
    "NO! The most common 10000 words\n",
    "\n",
    "How?\n",
    "- count the frequency of all the tokens\n",
    "- sort it and choose top 10,000\n",
    "- turn text to IDs based on this. For the rejected words, turn them into something like 'UNKNOWN'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:55.601999Z",
     "start_time": "2023-01-18T12:43:55.596804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 1, 'red': 1, 'pill': 1})\n",
      "Counter({'the': 2, 'red': 1, 'pill': 1, 'blue': 1, 'gill': 1})\n"
     ]
    }
   ],
   "source": [
    "# Understanding Counters\n",
    "counter = Counter()\n",
    "\n",
    "counter.update(['the', 'red', 'pill'])\n",
    "print(counter)\n",
    "counter.update(['the', 'blue', 'gill'])\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:56.343416Z",
     "start_time": "2023-01-18T12:43:55.603783Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "294721c361194122b6d4e2298d66fe59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counter = Counter()\n",
    "for document in tqdm(tokenized_train_text):\n",
    "    counter.update(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:56.369648Z",
     "start_time": "2023-01-18T12:43:56.344954Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121064,\n",
       " [('the', 289838),\n",
       "  (',', 275296),\n",
       "  ('.', 236702),\n",
       "  ('and', 156484),\n",
       "  ('a', 156282),\n",
       "  ('of', 144056),\n",
       "  ('to', 133886),\n",
       "  ('is', 109095),\n",
       "  ('in', 87676),\n",
       "  ('I', 77546)])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counter), counter.most_common(10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T10:03:57.020889Z",
     "start_time": "2023-01-18T10:03:56.409805Z"
    }
   },
   "source": [
    "## Let's plot the word frequencies and decide on a reasonable limit\n",
    "word_counts = [counter[word] for word in counter]\n",
    "bin_ranges = [i for i in range(0, max(word_counts), max(word_counts)//5000)]\n",
    "\n",
    "#create histogram with 4 bins\n",
    "print(f\"Over {len(lens)} words, the mean is {np.mean(word_counts):.2f} ± {np.std(word_counts):.2f}\")\n",
    "# plt.figure(figsize=(14, 8))\n",
    "# plt.hist(lens, bins=bin_ranges, edgecolor='black')\n",
    "# plt.plot()\n",
    "\n",
    "x = pd.Series(word_counts)\n",
    "\n",
    "# histogram on linear scale\n",
    "plt.subplot(211)\n",
    "hist, bins, _ = plt.hist(x, bins=50)\n",
    "\n",
    "# histogram on log scale. \n",
    "# Use non-equal bin sizes, such that they look equal on log scale.\n",
    "logbins = np.logspace(np.log10(bins[0]),np.log10(bins[-1]),len(bins))\n",
    "plt.figure(figsize=(14, 14))\n",
    "plt.subplot(212)\n",
    "plt.hist(x, bins=logbins)\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:56.372754Z",
     "start_time": "2023-01-18T12:43:56.370897Z"
    }
   },
   "outputs": [],
   "source": [
    "n_words = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:56.434802Z",
     "start_time": "2023-01-18T12:43:56.373794Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lets create the actual vocab now. \n",
    "# We need one special word for 'UNKNOWN': those words that our 'out of vocabulary' for us\n",
    "# and for 'PADDING': when a sequence is less than the seuqence length we decided\n",
    "vocab = {'--UNK--': 0, '--PAD--': 1} \n",
    "\n",
    "for i, (k,v) in enumerate(counter.most_common(n_words)):\n",
    "    vocab.setdefault(k, len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:56.438930Z",
     "start_time": "2023-01-18T12:43:56.436720Z"
    }
   },
   "outputs": [],
   "source": [
    "n_words = n_words + 2 # for special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T09:56:50.727278Z",
     "start_time": "2023-01-18T09:56:50.687945Z"
    }
   },
   "source": [
    "!! **Good idea to go through the vocabulary, spot the fishy ones and re-adapt your preprocessing to take care of them.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Coverting tokens to word IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:58.082950Z",
     "start_time": "2023-01-18T12:43:56.440647Z"
    }
   },
   "outputs": [],
   "source": [
    "wordid_train_text = [[vocab.get(tok, vocab['--UNK--']) for tok in doc] for doc in tokenized_train_text]\n",
    "bow_train_text = [list(set(doc)) for doc in wordid_train_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:58.314333Z",
     "start_time": "2023-01-18T12:43:58.084191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:            15G         11G        2,3G        690M        1,3G        2,7G\r\n",
      "Swap:          979M        979M        376K\r\n"
     ]
    }
   ],
   "source": [
    "! free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:58.787129Z",
     "start_time": "2023-01-18T12:43:58.317968Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 10002), (25000, 1), dtype('float32'), dtype('float32'))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.zeros((len(bow_train_text), n_words), dtype=np.float32)\n",
    "for i, doc in enumerate(bow_train_text):\n",
    "    X[i][doc] = 1\n",
    "Y = np.asarray(train_labels, dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "X.shape, Y.shape, X.dtype, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:59.673957Z",
     "start_time": "2023-01-18T12:43:58.788272Z"
    }
   },
   "outputs": [],
   "source": [
    "with Path('../resources/6.1.X.np').open('wb+') as f:\n",
    "    np.save(f, X)\n",
    "    \n",
    "with Path('../resources/6.1.Y.np').open('wb+') as f:\n",
    "    np.save(f, Y)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
