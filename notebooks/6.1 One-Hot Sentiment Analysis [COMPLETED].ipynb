{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "X -> Y makes supervised machine learning. We tried with random numbers. We tried with images. Pixels are numbers. Everything is numbers.\n",
    "\n",
    "We can treat text the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:09:50.628062Z",
     "start_time": "2023-01-26T12:09:50.507653Z"
    }
   },
   "outputs": [],
   "source": [
    "! free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:25:13.629027Z",
     "start_time": "2023-01-26T12:25:11.817936Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:09:54.855771Z",
     "start_time": "2023-01-26T12:09:51.765590Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the dataset\n",
    "imdb = load_dataset(\"imdb\")\n",
    "\n",
    "len(imdb['train']), imdb['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Vectors\n",
    "\n",
    "![One Hot](https://miro.medium.com/max/828/1*9ZuDXoc2ek-GfHE2esty5A.webp)\n",
    "src - https://medium.com/intelligentmachines/word-embedding-and-one-hot-encoding-ad17b4bbe111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:09:54.860580Z",
     "start_time": "2023-01-26T12:09:54.857214Z"
    }
   },
   "outputs": [],
   "source": [
    "document = \"A girl called Siyana had a little lamb\".lower()\n",
    "tokens = document.split(' ')\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:09:54.866251Z",
     "start_time": "2023-01-26T12:09:54.862333Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "for token in tokens:\n",
    "    token = token.lower()\n",
    "    if not token in vocab:\n",
    "        vocab[token] = len(vocab)\n",
    "        \n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:09:54.871872Z",
     "start_time": "2023-01-26T12:09:54.867830Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "for token in tokens:\n",
    "    vocab.setdefault(token, len(vocab))\n",
    "        \n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:09:54.877330Z",
     "start_time": "2023-01-26T12:09:54.873172Z"
    }
   },
   "outputs": [],
   "source": [
    "one_hots = []\n",
    "one_hots = np.zeros((len(vocab), len(tokens)))\n",
    "for word_nr, token in enumerate(tokens):\n",
    "    word_id = vocab[token]\n",
    "    one_hots[word_id, word_nr] = 1\n",
    "    \n",
    "one_hots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T10:18:20.080338Z",
     "start_time": "2023-01-18T10:18:20.075071Z"
    }
   },
   "source": [
    "## Multi-Hot Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:09:55.681882Z",
     "start_time": "2023-01-26T12:09:54.894415Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lets just work with 1000 documents for now\n",
    "\n",
    "train_text = [instance['text'] for instance in imdb['train']]#[:1000]\n",
    "train_labels = [instance['label'] for instance in imdb['train']]#[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess (estimate: 30-40 minutes).\n",
    "\n",
    "This is the most difficult part ^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenize Text\n",
    "\n",
    "Document is one long string of text -> One unit (pixel) can be a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:09:55.827179Z",
     "start_time": "2023-01-26T12:09:55.824451Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(document):\n",
    "    \"\"\"\n",
    "        1. lowercase everything \n",
    "    \"\"\"\n",
    "    document = document.replace(\".\", \" .\").replace(\"!\", \" !\")\n",
    "    return document.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:09:56.748912Z",
     "start_time": "2023-01-26T12:09:56.742693Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test our basic tokenizer\n",
    "'|'.join(tokenize(imdb['train'][0]['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Tokenizer\n",
    "This one is actually useful.\n",
    "\n",
    "### To Install It\n",
    "\n",
    "`! pip install spacy`\n",
    "\n",
    "`! python -m spacy download en_core_web_sm`\n",
    "\n",
    "within jupyter cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:10:01.005438Z",
     "start_time": "2023-01-26T12:09:59.051965Z"
    }
   },
   "outputs": [],
   "source": [
    "# Actually useful tokenizer\n",
    "import spacy\n",
    "exclude = [\"parser\", \"tagger\", \"ner\", \"textcat\", \"attribute_ruler\", \"lemmatizer\"]\n",
    "nlp = spacy.load(\"en_core_web_sm\", exclude=exclude)\n",
    "\n",
    "def get_spacy_tokens(text):\n",
    "    return [token.text for token in nlp(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:10:01.022433Z",
     "start_time": "2023-01-26T12:10:01.006481Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test Spacy Tokenizer\n",
    "tokens = get_spacy_tokens(train_text[0])\n",
    "'|'.join(tokens)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T21:55:28.747463Z",
     "start_time": "2023-01-17T21:55:25.159926Z"
    }
   },
   "source": [
    "# Tokenize Everything \n",
    "# NOTE: this might take some time. There are ways to speed it up but we dont need that for now\n",
    "tokenized_train_text = []\n",
    "for text in tqdm(train_text):\n",
    "   tokenized_train_text.append(get_spacy_tokens(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:10:05.236166Z",
     "start_time": "2023-01-26T12:10:05.091774Z"
    }
   },
   "outputs": [],
   "source": [
    "! free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:12:35.211210Z",
     "start_time": "2023-01-26T12:10:09.742433Z"
    }
   },
   "outputs": [],
   "source": [
    "# This takes 2-5 minutes. We'll talk till then ^^'\n",
    "\n",
    "train_docs = list(nlp.pipe(train_text))\n",
    "tokenized_train_text = [[tok.text for tok in doc] for doc in train_docs]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T09:47:52.045634Z",
     "start_time": "2023-01-18T09:47:52.040144Z"
    }
   },
   "source": [
    "## 1.1 Exploring the data \n",
    "\n",
    "- Length Distribution\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T09:54:58.791484Z",
     "start_time": "2023-01-18T09:54:58.573048Z"
    }
   },
   "source": [
    "lens = [len(doc) for doc in tokenized_train_text]\n",
    "bin_ranges = [i for i in range(0, max(lens), max(lens)//50)]\n",
    "\n",
    "#create histogram with 4 bins\n",
    "print(f\"Over {len(lens)} documents, the mean is {np.mean(lens):.2f} ± {np.std(lens):.2f}\")\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.hist(lens, bins=bin_ranges, edgecolor='black')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T09:55:22.669868Z",
     "start_time": "2023-01-18T09:55:22.664095Z"
    }
   },
   "source": [
    "# Lets decide on a maximum length of documents based on this. Say 200\n",
    "maxlen = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:12:36.598980Z",
     "start_time": "2023-01-26T12:12:35.212435Z"
    }
   },
   "outputs": [],
   "source": [
    "# The same setdefault stuff we did above\n",
    "vocab = {}\n",
    "for document in tqdm(tokenized_train_text):\n",
    "    for token in document:\n",
    "        vocab.setdefault(token, len(vocab))\n",
    "    \n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T22:07:52.341213Z",
     "start_time": "2023-01-17T22:07:52.334930Z"
    }
   },
   "source": [
    "### That's way too many words. 121064?\n",
    "\n",
    "Let's make sure we have only 10000 words. First 10000 words?\n",
    "NO! The most common 10000 words\n",
    "\n",
    "How?\n",
    "- count the frequency of all the tokens\n",
    "- sort it and choose top 10,000\n",
    "- turn text to IDs based on this. For the rejected words, turn them into something like 'UNKNOWN'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:12:36.602910Z",
     "start_time": "2023-01-26T12:12:36.600256Z"
    }
   },
   "outputs": [],
   "source": [
    "# Understanding Counters\n",
    "counter = Counter()\n",
    "\n",
    "counter.update(['the', 'red', 'pill'])\n",
    "print(counter)\n",
    "counter.update(['the', 'blue', 'gill'])\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:12:37.341622Z",
     "start_time": "2023-01-26T12:12:36.603922Z"
    }
   },
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "for document in tqdm(tokenized_train_text):\n",
    "    counter.update(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:12:37.363896Z",
     "start_time": "2023-01-26T12:12:37.342956Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(counter), counter.most_common(10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T10:03:57.020889Z",
     "start_time": "2023-01-18T10:03:56.409805Z"
    }
   },
   "source": [
    "## Let's plot the word frequencies and decide on a reasonable limit\n",
    "word_counts = [counter[word] for word in counter]\n",
    "bin_ranges = [i for i in range(0, max(word_counts), max(word_counts)//5000)]\n",
    "\n",
    "#create histogram with 4 bins\n",
    "print(f\"Over {len(lens)} words, the mean is {np.mean(word_counts):.2f} ± {np.std(word_counts):.2f}\")\n",
    "# plt.figure(figsize=(14, 8))\n",
    "# plt.hist(lens, bins=bin_ranges, edgecolor='black')\n",
    "# plt.plot()\n",
    "\n",
    "x = pd.Series(word_counts)\n",
    "\n",
    "# histogram on linear scale\n",
    "plt.subplot(211)\n",
    "hist, bins, _ = plt.hist(x, bins=50)\n",
    "\n",
    "# histogram on log scale. \n",
    "# Use non-equal bin sizes, such that they look equal on log scale.\n",
    "logbins = np.logspace(np.log10(bins[0]),np.log10(bins[-1]),len(bins))\n",
    "plt.figure(figsize=(14, 14))\n",
    "plt.subplot(212)\n",
    "plt.hist(x, bins=logbins)\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:12:37.366828Z",
     "start_time": "2023-01-26T12:12:37.365019Z"
    }
   },
   "outputs": [],
   "source": [
    "n_words = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:18:01.953487Z",
     "start_time": "2023-01-26T12:18:01.880983Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lets create the actual vocab now. \n",
    "# We need one special word for 'UNKNOWN': those words that our 'out of vocabulary' for us\n",
    "# and for 'PADDING': when a sequence is less than the seuqence length we decided\n",
    "vocab = {'--UNK--': 0, '--PAD--': 1} \n",
    "\n",
    "for i, (k,v) in enumerate(counter.most_common(n_words)):\n",
    "    vocab.setdefault(k, len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:18:02.205255Z",
     "start_time": "2023-01-26T12:18:02.200671Z"
    }
   },
   "outputs": [],
   "source": [
    "n_words = n_words + 2 # for special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T09:56:50.727278Z",
     "start_time": "2023-01-18T09:56:50.687945Z"
    }
   },
   "source": [
    "!! **Good idea to go through the vocabulary, spot the fishy ones and re-adapt your preprocessing to take care of them.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Coverting tokens to word IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:18:05.790719Z",
     "start_time": "2023-01-26T12:18:05.045680Z"
    }
   },
   "outputs": [],
   "source": [
    "wordid_train_text = [[vocab.get(tok, vocab['--UNK--']) for tok in doc] for doc in tokenized_train_text]\n",
    "# bow_train_text = [list(set(doc)) for doc in wordid_train_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:18:58.030255Z",
     "start_time": "2023-01-26T12:18:57.790273Z"
    }
   },
   "outputs": [],
   "source": [
    "! free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:18:17.059967Z",
     "start_time": "2023-01-26T12:18:17.054071Z"
    }
   },
   "source": [
    "### 3.2 Do the same for test text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:22:04.826599Z",
     "start_time": "2023-01-26T12:19:40.992063Z"
    }
   },
   "outputs": [],
   "source": [
    "test_text = [instance['text'] for instance in imdb['test']]#[:1000]\n",
    "test_labels = [instance['label'] for instance in imdb['test']]#[:1000]\n",
    "test_docs = list(nlp.pipe(test_text))\n",
    "tokenized_test_text = [[tok.text for tok in doc] for doc in test_docs]\n",
    "wordid_test_text = [[vocab.get(tok, vocab['--UNK--']) for tok in doc] for doc in tokenized_test_text]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-24T12:40:00.782526Z",
     "start_time": "2023-01-24T12:40:00.774575Z"
    }
   },
   "source": [
    "## 4. Dump this stuff to disk\n",
    "\n",
    "Next step is a transformation where we lose information (i.e. cant get sequence back from Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:23:45.039031Z",
     "start_time": "2023-01-26T12:23:44.757272Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dump the WordID and vocab to disk\n",
    "dump_dir = Path('../resources/datasets/imdb/wordid_vocab')\n",
    "dump_dir.mkdir(parents=True, exist_ok=True)\n",
    "with (dump_dir/'vocab.json').open('w+') as f:\n",
    "    json.dump(vocab, f)\n",
    "    \n",
    "with (dump_dir/'wordids_train.pkl').open('wb+') as f:\n",
    "    pickle.dump(wordid_train_text, f)\n",
    "    \n",
    "with (dump_dir/'train_labels.pkl').open('wb+') as f:\n",
    "    pickle.dump(train_labels, f)\n",
    "    \n",
    "with (dump_dir/'wordids_test.pkl').open('wb+') as f:\n",
    "    pickle.dump(wordid_test_text, f)\n",
    "    \n",
    "with (dump_dir/'test_labels.pkl').open('wb+') as f:\n",
    "    pickle.dump(test_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:26:44.993958Z",
     "start_time": "2023-01-26T12:26:44.247121Z"
    }
   },
   "outputs": [],
   "source": [
    "# Try loading from disk\n",
    "dump_dir = Path('../resources/datasets/imdb/wordid_vocab')\n",
    "with (dump_dir/'vocab.json').open('r') as f:\n",
    "    vocab = json.load(f)\n",
    "    \n",
    "with (dump_dir/'wordids_train.pkl').open('rb') as f:\n",
    "    wordid_train_text = pickle.load(f)\n",
    "    \n",
    "with (dump_dir/'train_labels.pkl').open('rb') as f:\n",
    "    train_labels = pickle.load(f)\n",
    "    \n",
    "with (dump_dir/'wordids_test.pkl').open('rb') as f:\n",
    "    wordid_test_text = pickle.load(f)\n",
    "    \n",
    "with (dump_dir/'test_labels.pkl').open('rb') as f:\n",
    "    test_labels = pickle.load(f)\n",
    "    \n",
    "n_words = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bag of Words\n",
    "\n",
    "We don't need sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:26:45.459781Z",
     "start_time": "2023-01-26T12:26:45.454543Z"
    }
   },
   "outputs": [],
   "source": [
    "# Case 1: Only one-hot representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:26:47.809161Z",
     "start_time": "2023-01-26T12:26:45.668147Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.013863911)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.zeros((len(wordid_train_text), n_words), dtype=np.float32)\n",
    "Y = np.asarray(train_labels, dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "for i, wordid_document in enumerate(wordid_train_text):\n",
    "    for token_id in wordid_document:\n",
    "        X[i][token_id] = 1\n",
    "    \n",
    "    \n",
    "X.max(), X.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:26:49.004356Z",
     "start_time": "2023-01-26T12:26:47.810150Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dump this to disk\n",
    "dump_dir = dump_dir.parent / 'bow_onehot'\n",
    "dump_dir.mkdir(parents=True, exist_ok=True)\n",
    "with (dump_dir / 'X_train.np').open('wb+') as f:\n",
    "    np.save(f, X)\n",
    "    \n",
    "with (dump_dir / 'Y_train.np').open('wb+') as f:\n",
    "    np.save(f, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:26:51.097421Z",
     "start_time": "2023-01-26T12:26:49.006071Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.013560608)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do the same for test stuff\n",
    "# Overwriting variable names to conserve RAM\n",
    "X = np.zeros((len(wordid_test_text), n_words), dtype=np.float32)\n",
    "Y = np.asarray(test_labels, dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "for i, wordid_document in enumerate(wordid_test_text):\n",
    "    for token_id in wordid_document:\n",
    "        X[i][token_id] = 1\n",
    "    \n",
    "    \n",
    "X.max(), X.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:26:51.681214Z",
     "start_time": "2023-01-26T12:26:51.098395Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dump this to disk\n",
    "dump_dir = dump_dir.parent / 'bow_onehot'\n",
    "dump_dir.mkdir(parents=True, exist_ok=True)\n",
    "with (dump_dir / 'X_test.np').open('wb+') as f:\n",
    "    np.save(f, X)\n",
    "    \n",
    "with (dump_dir / 'Y_test.np').open('wb+') as f:\n",
    "    np.save(f, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:26:51.684281Z",
     "start_time": "2023-01-26T12:26:51.682328Z"
    }
   },
   "outputs": [],
   "source": [
    "# Case 2: MultiHot Representations (with frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:27:07.483249Z",
     "start_time": "2023-01-26T12:26:51.685531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(454.0, 0.02724074)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.zeros((len(wordid_train_text), n_words), dtype=np.float32)\n",
    "Y = np.asarray(train_labels, dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "for i, wordid_document in enumerate(wordid_train_text):\n",
    "    for token_id in wordid_document:\n",
    "        X[i][token_id] += 1\n",
    "    \n",
    "X.max(), X.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:27:08.639184Z",
     "start_time": "2023-01-26T12:27:07.484391Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dump this to disk\n",
    "dump_dir = dump_dir.parent / 'bow_multihot'\n",
    "dump_dir.mkdir(parents=True, exist_ok=True)\n",
    "with (dump_dir / 'X_train.np').open('wb+') as f:\n",
    "    np.save(f, X)\n",
    "    \n",
    "with (dump_dir / 'Y_train.np').open('wb+') as f:\n",
    "    np.save(f, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:27:25.321508Z",
     "start_time": "2023-01-26T12:27:08.640511Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(294.0, 0.02662703)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do the same for test stuff\n",
    "# Overwriting variable names to conserve RAM\n",
    "X = np.zeros((len(wordid_test_text), n_words), dtype=np.float32)\n",
    "Y = np.asarray(test_labels, dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "for i, wordid_document in enumerate(wordid_test_text):\n",
    "    for token_id in wordid_document:\n",
    "        X[i][token_id] += 1\n",
    "    \n",
    "    \n",
    "X.max(), X.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T12:27:25.855646Z",
     "start_time": "2023-01-26T12:27:25.322749Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dump this to disk\n",
    "dump_dir = dump_dir.parent / 'bow_multihot'\n",
    "dump_dir.mkdir(parents=True, exist_ok=True)\n",
    "with (dump_dir / 'X_test.np').open('wb+') as f:\n",
    "    np.save(f, X)\n",
    "    \n",
    "with (dump_dir / 'Y_test.np').open('wb+') as f:\n",
    "    np.save(f, Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
