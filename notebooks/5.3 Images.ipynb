{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "train = datasets.MNIST(\"\", train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# Testing data\n",
    "test = datasets.MNIST(\"\", train=False, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=10, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def viz_img(mat):\n",
    "    if mat.shape.__len__() == 3:\n",
    "        mat = mat.squeeze(0)\n",
    "        \n",
    "    plt.imshow(mat.numpy())\n",
    "\n",
    "def viz_pred(logits):\n",
    "    plt.figure()\n",
    "    plt.bar(np.arange(0, 10), logits.squeeze(0).detach().numpy())\n",
    "    plt.xticks(np.arange(0, 10))\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the image and the label\n",
    "i = torch.randint(0, len(train)-1, (1,)).item()\n",
    "viz_img(train[i][0])\n",
    "\n",
    "print(f'Idx: {i:6d} Corresponding label: ', train[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinCLf(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim: int, n_classes: int):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NonLinCLf(784, 10)\n",
    "lfn = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in trange(epochs):\n",
    "    for batch in train_loader:\n",
    "        inputs, target = batch\n",
    "        opt.zero_grad()\n",
    "        preds = model(inputs.reshape(-1, 784))\n",
    "        loss = lfn(preds, target)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 404\n",
    "print(test[i][0].shape, test[i][1])\n",
    "viz_img(test[i][0])\n",
    "\n",
    "pred = model(test[i][0].reshape(-1, 28*28))\n",
    "viz_pred(pred)\n",
    "viz_pred(F.softmax(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets calculate its real performance?\n",
    "test_data = test.test_data.reshape(-1, 784).to(torch.float32)\n",
    "test_logits = model(test_data)\n",
    "test_labels = test.test_labels\n",
    "\n",
    "# How do we use this to calc acc?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets look at another very famous dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. Load the dataset from Hugging Face\n",
    "ds = load_dataset(\"uoft-cs/cifar10\")\n",
    "train_ds = ds['train']\n",
    "test_ds = ds['test']\n",
    "\n",
    "# 2. Define a simple transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor() # Converts PIL image to PyTorch tensor and scales to [0,1]\n",
    "])\n",
    "\n",
    "# 3. Create a function to apply transformations\n",
    "def apply_transforms(examples):\n",
    "    examples['inputs'] = [transform(image.convert(\"RGB\")) for image in examples['img']]\n",
    "    return examples\n",
    "\n",
    "# 4. Apply transformations to the datasets\n",
    "train_ds.set_transform(apply_transforms, columns=['img', 'label'], output_all_columns=False)\n",
    "test_ds.set_transform(apply_transforms, columns=['img', 'label'], output_all_columns=False)\n",
    "\n",
    "\n",
    "# 5. Define a collate function\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([example['inputs'] for example in batch])\n",
    "    labels = torch.tensor([example['label'] for example in batch])\n",
    "    return {'inputs': pixel_values, 'label': labels}\n",
    "\n",
    "# 6. Create DataLoaders\n",
    "train_loader = DataLoader(train_ds, batch_size=10, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_ds, batch_size=10, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    images = batch['inputs']\n",
    "    labels = batch['label']\n",
    "    print(f\"Batch {batch_idx + 1}:\")\n",
    "    print(\"Images shape:\", images.shape) # Should be [batch_size, 3, 32, 32]\n",
    "    print(\"Labels shape:\", labels.shape)   # Should be [batch_size]\n",
    "    print(\"Labels in the batch:\", labels)\n",
    "    if batch_idx == 0: # Check only the first batch\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'].features['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(ds['train'][20]['img']).shape, ds['train'][20]['label'])\n",
    "ds['train'][20]['img']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## how many labels\n",
    "ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing dataloaders\n",
    "for batch in train_loader:\n",
    "    inputs, label = batch.values()\n",
    "    break\n",
    "\n",
    "inputs.shape, label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on it\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logits = []\n",
    "test_labels = []\n",
    "for batch in tqdm(test_loader):\n",
    "    inputs, target = batch.values()\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs.reshape(-1, 3*32*32))\n",
    "    test_logits.append(logits)\n",
    "    test_labels.append(target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logits = torch.cat(test_logits)\n",
    "test_labels = torch.cat(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What did we do here?\n",
    "\n",
    "We flattened the image out into a set of pixels. Each pixel contained some information (value between 0 and 1) but it also had some information based on its neighbours. We took away this information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What else can we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets load an image\n",
    "from pathlib import Path\n",
    "imgpath = Path('../resources/grumpycat.jpg')\n",
    "assert imgpath.exists()\n",
    "\n",
    "ANIMATION_PAUSE_DURATION = 0.0005\n",
    "target_image_size = (60, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageOps # Pillow for image manipulation\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from IPython import display # For updating the plot in Jupyter\n",
    "import time # For plt.pause\n",
    "def prepimage(image_path_str, target_size):\n",
    "    \"\"\"\n",
    "    Loads an image, converts it to grayscale, resizes it, and returns it as a NumPy array.\n",
    "    If the image is not found, a placeholder gradient image is created.\n",
    "    \"\"\"\n",
    "    # Open the image using Pillow\n",
    "    img = Image.open(image_path_str)\n",
    "    # Convert to grayscale ('L' mode)\n",
    "    img_gray = img.convert('L')\n",
    "    # Resize and crop to the target size to maintain aspect ratio\n",
    "    img_resized = ImageOps.fit(img_gray, target_size, Image.Resampling.LANCZOS)\n",
    "    # Convert the PIL image to a NumPy array\n",
    "    return np.array(img_resized)\n",
    "\n",
    "\n",
    "imgnp = prepimage(imgpath, target_image_size)\n",
    "img_height, img_width = imgnp.shape\n",
    "viz_img(torch.tensor(imgnp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel = np.array([\n",
    "#     [-1, -2, -1],\n",
    "#     [ 0,  0,  0],\n",
    "#     [ 1,  2,  1]\n",
    "# ])\n",
    "\n",
    "kernel = np.array([\n",
    "    [-1, -1, -1, -1, -1],\n",
    "    [-1, -1, -1, -1, -1],\n",
    "    [ 0,  0,  0,  0,  0],\n",
    "    [ 1,  1,  1,  1,  1],\n",
    "    [ 1,  1,  1,  1,  1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Feature Map Initialization\n",
    "\n",
    "k_height, k_width = kernel.shape\n",
    "output_height = img_height - k_height + 1\n",
    "output_width = img_width - k_width + 1\n",
    "\n",
    "# Initialize the output feature map with zeros\n",
    "output_feature_map = np.zeros((output_height, output_width))\n",
    "\n",
    "# Some book keeping\n",
    "print(\"Starting convolution visualization...\")\n",
    "print(f\"Input image size: {imgnp.shape}\")\n",
    "print(f\"Kernel size: {kernel.shape}\")\n",
    "print(f\"Output map size: {output_feature_map.shape}\")\n",
    "\n",
    "# Determine a reasonable color scale for the output. Sobel can produce negative values.\n",
    "# Max possible activation value (absolute) can be estimated.\n",
    "max_abs_activation = np.sum(np.abs(kernel)) * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each position where the kernel can be placed on the input image\n",
    "for y_out in range(output_height):\n",
    "    for x_out in range(output_width):\n",
    "        # y_in, x_in are the top-left coordinates of the kernel's current window on the input image\n",
    "        y_in = y_out\n",
    "        x_in = x_out\n",
    "\n",
    "        # Extract the region of interest (ROI) from the input image\n",
    "        # This is the patch of the image that the kernel is currently overlapping\n",
    "        roi = imgnp[y_in : y_in + k_height, x_in : x_in + k_width]\n",
    "\n",
    "        # Perform the core convolution operation:\n",
    "        # Element-wise multiplication of the ROI and the kernel, then sum the results\n",
    "        activation = np.sum(roi * kernel)\n",
    "\n",
    "        # Store the calculated activation in the output feature map\n",
    "        output_feature_map[y_out, x_out] = activation\n",
    "\n",
    "        # --- Update Visualization (Recreate plot in each step) ---\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "        # Subplot 1: Input image and the sliding kernel box\n",
    "        ax1.set_title(f\"Input (Step {y_out*output_width + x_out + 1}/{output_height*output_width})\")\n",
    "        ax1.set_xticks([])\n",
    "        ax1.set_yticks([])\n",
    "        ax1.imshow(imgnp, cmap='gray', vmin=0, vmax=255)\n",
    "        kernel_box = patches.Rectangle((x_in - 0.5, y_in - 0.5), k_width, k_height,\n",
    "                                       linewidth=1.5, edgecolor='r', facecolor='none')\n",
    "        ax1.add_patch(kernel_box)\n",
    "\n",
    "        # Subplot 2: Output feature map (activations)\n",
    "        ax2.set_title(\"Output Feature Map\")\n",
    "        ax2.set_xticks([])\n",
    "        ax2.set_yticks([])\n",
    "        ax2.imshow(output_feature_map, cmap='coolwarm',\n",
    "                   vmin=-max_abs_activation, vmax=max_abs_activation)\n",
    "\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # # Pause to control animation speed\n",
    "        # if ANIMATION_PAUSE_DURATION > 0:\n",
    "        #     time.sleep(ANIMATION_PAUSE_DURATION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing it in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgClf(nn.Module):\n",
    "    def __init__(self, n_classes: int=10):\n",
    "        super().__init__()\n",
    "        # Input: bs x 3 x 32 x 32 (R G B and each having 32 x 32 pixels)\n",
    "\n",
    "        # Conv layer 1: Input: bs x 3 x 32 x 32. Output: bs x 16 x 32 x 32\n",
    "        self.conv1 = \n",
    "        # Will do pooling in forward so Output bs x 16 x 16 x 16\n",
    "\n",
    "        # Conv layer 2: Input: bs x 16 x 16 x 16. Output: bs x 32 x 16 x 16\n",
    "        self.conv2 = \n",
    "        # Will do pooling in forward so Output bs x 16 x 8 x 8\n",
    "\n",
    "        # Define the pooling\n",
    "        self.pool = \n",
    "\n",
    "        # Then we do MLP over this representation of data\n",
    "        self.mlp1 = \n",
    "        self.mlp2 = \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # do the two convs\n",
    "\n",
    "        # Flatten\n",
    "\n",
    "        # Apply MLP and activations\n",
    "        return \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImgClf(10)\n",
    "lfn = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on it\n",
    "for epoch in trange(epochs):\n",
    "    for batch in train_loader:\n",
    "        opt.zero_grad()\n",
    "        inputs, label = batch.values()\n",
    "        preds = model(inputs)\n",
    "        loss = lfn(preds, label)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing?\n",
    "test_logits = []\n",
    "test_labels = []\n",
    "for batch in tqdm(test_loader):\n",
    "    inputs, target = batch.values()\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs.reshape(-1, 3*32*32))\n",
    "    test_logits.append(logits)\n",
    "    test_labels.append(target)\n",
    "\n",
    "test_logits = torch.cat(test_logits)\n",
    "test_labels = torch.cat(test_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
