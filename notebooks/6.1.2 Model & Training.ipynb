{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:42:46.157170Z",
     "start_time": "2023-01-18T12:42:34.482515Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm, trange\n",
    "from typing import Callable, Type\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revision\n",
    "\n",
    "**Goal**: We want to classify sentiments in a document.\n",
    "\n",
    "**Problem**: How do we convert a document into a set of numbers?\n",
    "\n",
    "- tokenize text\n",
    "- make vocab\n",
    "- convert word to IDs\n",
    "- use them to make a bag of words document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def acc(y_pred, y_true):\n",
    "    return ((y_pred > 0.5).int() == y_true).float().mean()\n",
    "\n",
    "def train(\n",
    "    model: torch.nn.Module,\n",
    "    X: torch.Tensor, \n",
    "    Y: torch.Tensor,\n",
    "    X_valid: torch.Tensor,\n",
    "    Y_valid: torch.Tensor,\n",
    "    loss_function: Callable = torch.nn.MSELoss(), \n",
    "    optimizer: Type = torch.optim.SGD,\n",
    "    epochs: int = 200,\n",
    "    batch_size: int = 32,\n",
    "    ) -> tuple[torch.nn.Module, list[float]]:\n",
    "    \n",
    "    dataloader = DataLoader(TensorDataset(X, Y), batch_size=batch_size, shuffle=True)\n",
    "    losses = []\n",
    "    training_accs = []\n",
    "    validation_accs = []\n",
    "    \n",
    "    for epoch in (pbar := tqdm(range(epochs + 1))):\n",
    "        epoch_loss = 0.0\n",
    "        epoch_training_acc = []\n",
    "        model.train()\n",
    "        for X_batch, Y_batch in dataloader:\n",
    "            # Do a train step\n",
    "            optimizer.zero_grad()\n",
    "            Y_pred = model(X_batch)\n",
    "            loss = loss_function(Y_pred, Y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate loss\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_training_acc.append(acc(Y_pred, Y_batch).item())\n",
    "    \n",
    "        # Calculate validation metrics\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            y_pred_valid = model(X_valid)\n",
    "            avg_vlacc = acc(y_pred_valid, Y_valid)\n",
    "        \n",
    "        # Log results\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        avg_tracc = sum(epoch_training_acc) / len(epoch_training_acc)\n",
    "        losses.append(avg_loss)\n",
    "        training_accs.append(avg_tracc)\n",
    "        validation_accs.append(avg_vlacc)\n",
    "\n",
    "\n",
    "\n",
    "        pbar.set_description(f\"Epoch {epoch}/{epochs} - Loss: {avg_loss:.6f} - Tracc: {avg_tracc:.3f} - Vlacc: {avg_vlacc:.3f}\")\n",
    "\n",
    "    # Visualize loss and accuracy\n",
    "    plt.figure(figsize=(15, 5), dpi=200)\n",
    "\n",
    "    # Plot training loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(losses, label='Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Training Loss (last_avg: {float(np.mean(losses[-len(losses)//10:])):.6f})')\n",
    "    plt.xticks()\n",
    "    plt.yticks()\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot training and validation accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(training_accs, label='Training Accuracy', color='orange')\n",
    "    plt.plot(validation_accs, label='Validation Accuracy', color='green')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'Accuracy (Training last_avg: {float(np.mean(training_accs[-len(training_accs)//10:])):.6f}, Validation last_avg: {float(np.mean(validation_accs[-len(validation_accs)//10:])):.6f})')\n",
    "    plt.xticks()\n",
    "    plt.yticks()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets load up word ids and vocab\n",
    "\n",
    "data_dir = Path('..') / 'resources' / 'datasets' / 'imdb' / 'proc'\n",
    "assert data_dir.exists()\n",
    "\n",
    "with (data_dir / 'wordids_train.pkl').open('rb') as f:\n",
    "    train_docs = pickle.load(f)\n",
    "\n",
    "with (data_dir / 'train_labels.pkl').open('rb') as f:\n",
    "    train_labels = pickle.load(f)\n",
    "\n",
    "with (data_dir / 'wordids_test.pkl').open('rb') as f:\n",
    "    test_docs = pickle.load(f)\n",
    "\n",
    "with (data_dir / 'test_labels.pkl').open('rb') as f:\n",
    "    test_labels = pickle.load(f)\n",
    "\n",
    "with (data_dir / 'vocab.json').open('r') as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "len(train_docs), len(test_docs), len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs = ...\n",
    "n_words = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do the bag of words representation really quickly\n",
    "X = ...\n",
    "Y = ...\n",
    "\n",
    "...\n",
    "\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:22:30.081412Z",
     "start_time": "2023-01-18T12:22:30.074834Z"
    }
   },
   "outputs": [],
   "source": [
    "# Shuffle the dataset (using np random permutation)\n",
    "p = np.random.permutation(len(X))\n",
    "X = ...\n",
    "Y = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:22:30.092254Z",
     "start_time": "2023-01-18T12:22:30.083199Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the dataset \n",
    "x_train, x_valid = ...\n",
    "y_train, y_valid = ...\n",
    "\n",
    "x_train.shape, x_valid.shape, y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brr lets never do it ourselves\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "...\n",
    "x_train.shape, x_valid.shape, y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:22:30.101391Z",
     "start_time": "2023-01-18T12:22:30.093405Z"
    }
   },
   "outputs": [],
   "source": [
    "class NonLin(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_words):\n",
    "        # Linear -> BatchNorm -> Dropout\n",
    "        ...\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # End with Sigmoid\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:22:30.114732Z",
     "start_time": "2023-01-18T12:22:30.102406Z"
    }
   },
   "outputs": [],
   "source": [
    "m = ...\n",
    "lfn = ... # BCE\n",
    "opt = # Adam with 0.005\n",
    "\n",
    "\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:22:30.120649Z",
     "start_time": "2023-01-18T12:22:30.116954Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, losses = train(...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
