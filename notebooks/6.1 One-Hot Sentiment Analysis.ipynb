{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "X -> Y makes supervised machine learning. We tried with random numbers. We tried with images. Pixels are numbers. Everything is numbers.\n",
    "\n",
    "We can treat text the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:30:52.261004Z",
     "start_time": "2023-01-18T12:30:50.663733Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:30:53.645065Z",
     "start_time": "2023-01-18T12:30:52.262473Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the dataset\n",
    "imdb = load_dataset(\"imdb\")\n",
    "\n",
    "len(imdb['train']), imdb['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Vectors\n",
    "\n",
    "![One Hot](https://miro.medium.com/max/828/1*9ZuDXoc2ek-GfHE2esty5A.webp)\n",
    "src - https://medium.com/intelligentmachines/word-embedding-and-one-hot-encoding-ad17b4bbe111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:30:53.650869Z",
     "start_time": "2023-01-18T12:30:53.647058Z"
    }
   },
   "outputs": [],
   "source": [
    "document = \"A girl called Siyana had a little lamb\".lower()\n",
    "tokens = document.split(' ')\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:30:53.656528Z",
     "start_time": "2023-01-18T12:30:53.652700Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "for token in tokens:\n",
    "    token = token.lower()\n",
    "    if not token in vocab:\n",
    "        vocab[token] = len(vocab)\n",
    "        \n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:30:53.661726Z",
     "start_time": "2023-01-18T12:30:53.657685Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "for token in tokens:\n",
    "    vocab.setdefault(token, len(vocab))\n",
    "        \n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:30:53.668976Z",
     "start_time": "2023-01-18T12:30:53.663427Z"
    }
   },
   "outputs": [],
   "source": [
    "one_hots = []\n",
    "one_hots = np.zeros((len(vocab), len(tokens)))\n",
    "for word_nr, token in enumerate(tokens):\n",
    "    word_id = vocab[token]\n",
    "    one_hots[word_id, word_nr] = 1\n",
    "    \n",
    "one_hots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T10:18:20.080338Z",
     "start_time": "2023-01-18T10:18:20.075071Z"
    }
   },
   "source": [
    "## Multi-Hot Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:41:08.713569Z",
     "start_time": "2023-01-18T12:41:07.963074Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lets just work with 1000 documents for now\n",
    "\n",
    "train_text = ...\n",
    "train_labels = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess (estimate: 30-40 minutes).\n",
    "\n",
    "This is the most difficult part ^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenize Text\n",
    "\n",
    "Document is one long string of text -> One unit (pixel) can be a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:41:09.518419Z",
     "start_time": "2023-01-18T12:41:09.508124Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(document):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:41:09.831954Z",
     "start_time": "2023-01-18T12:41:09.826428Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test our basic tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Tokenizer\n",
    "This one is actually useful.\n",
    "\n",
    "### To Install It\n",
    "\n",
    "`! pip install spacy`\n",
    "\n",
    "`! python -m spacy download en_core_web_sm`\n",
    "\n",
    "within jupyter cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:41:10.663247Z",
     "start_time": "2023-01-18T12:41:10.182161Z"
    }
   },
   "outputs": [],
   "source": [
    "# Actually useful tokenizer\n",
    "import spacy\n",
    "exclude = [\"parser\", \"tagger\", \"ner\", \"textcat\", \"attribute_ruler\", \"lemmatizer\"]\n",
    "nlp = spacy.load(\"en_core_web_sm\", exclude=exclude)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:41:10.677249Z",
     "start_time": "2023-01-18T12:41:10.664693Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test Spacy Tokenizer\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T21:55:28.747463Z",
     "start_time": "2023-01-17T21:55:25.159926Z"
    }
   },
   "source": [
    "# Tokenize Everything \n",
    "# NOTE: this might take some time. There are ways to speed it up but we dont need that for now\n",
    "tokenized_train_text = []\n",
    "for text in tqdm(train_text):\n",
    "   tokenized_train_text.append(get_spacy_tokens(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:53.826901Z",
     "start_time": "2023-01-18T12:41:10.724969Z"
    }
   },
   "outputs": [],
   "source": [
    "# This takes 2-5 minutes. We'll talk till then ^^'\n",
    "\n",
    "train_docs = ...  # use NLP pipe\n",
    "tokenized_train_text = ..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T09:47:52.045634Z",
     "start_time": "2023-01-18T09:47:52.040144Z"
    }
   },
   "source": [
    "## 1.1 Exploring the data \n",
    "\n",
    "- Length Distribution\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T09:54:58.791484Z",
     "start_time": "2023-01-18T09:54:58.573048Z"
    }
   },
   "source": [
    "lens = [len(doc) for doc in tokenized_train_text]\n",
    "bin_ranges = [i for i in range(0, max(lens), max(lens)//50)]\n",
    "\n",
    "#create histogram with 4 bins\n",
    "print(f\"Over {len(lens)} documents, the mean is {np.mean(lens):.2f} Â± {np.std(lens):.2f}\")\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.hist(lens, bins=bin_ranges, edgecolor='black')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T09:55:22.669868Z",
     "start_time": "2023-01-18T09:55:22.664095Z"
    }
   },
   "source": [
    "# Lets decide on a maximum length of documents based on this. Say 200\n",
    "maxlen = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:55.594901Z",
     "start_time": "2023-01-18T12:43:53.828333Z"
    }
   },
   "outputs": [],
   "source": [
    "# The same setdefault stuff we did above\n",
    "vocab = {}\n",
    "    ...\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T22:07:52.341213Z",
     "start_time": "2023-01-17T22:07:52.334930Z"
    }
   },
   "source": [
    "### That's way too many words. 121064?\n",
    "\n",
    "Let's make sure we have only 10000 words. First 10000 words?\n",
    "NO! The most common 10000 words\n",
    "\n",
    "How?\n",
    "- count the frequency of all the tokens\n",
    "- sort it and choose top 10,000\n",
    "- turn text to IDs based on this. For the rejected words, turn them into something like 'UNKNOWN'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:55.601999Z",
     "start_time": "2023-01-18T12:43:55.596804Z"
    }
   },
   "outputs": [],
   "source": [
    "# Understanding Counters\n",
    "counter = Counter()\n",
    "\n",
    "counter.update(['the', 'red', 'pill'])\n",
    "print(counter)\n",
    "counter.update(['the', 'blue', 'gill'])\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:56.343416Z",
     "start_time": "2023-01-18T12:43:55.603783Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run a counter over our tokenized dataset\n",
    "counter = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:56.369648Z",
     "start_time": "2023-01-18T12:43:56.344954Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's see what turned out (checkout most_common)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T10:03:57.020889Z",
     "start_time": "2023-01-18T10:03:56.409805Z"
    }
   },
   "source": [
    "## Let's plot the word frequencies and decide on a reasonable limit\n",
    "word_counts = [counter[word] for word in counter]\n",
    "bin_ranges = [i for i in range(0, max(word_counts), max(word_counts)//5000)]\n",
    "\n",
    "#create histogram with 4 bins\n",
    "print(f\"Over {len(lens)} words, the mean is {np.mean(word_counts):.2f} Â± {np.std(word_counts):.2f}\")\n",
    "# plt.figure(figsize=(14, 8))\n",
    "# plt.hist(lens, bins=bin_ranges, edgecolor='black')\n",
    "# plt.plot()\n",
    "\n",
    "x = pd.Series(word_counts)\n",
    "\n",
    "# histogram on linear scale\n",
    "plt.subplot(211)\n",
    "hist, bins, _ = plt.hist(x, bins=50)\n",
    "\n",
    "# histogram on log scale. \n",
    "# Use non-equal bin sizes, such that they look equal on log scale.\n",
    "logbins = np.logspace(np.log10(bins[0]),np.log10(bins[-1]),len(bins))\n",
    "plt.figure(figsize=(14, 14))\n",
    "plt.subplot(212)\n",
    "plt.hist(x, bins=logbins)\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:56.372754Z",
     "start_time": "2023-01-18T12:43:56.370897Z"
    }
   },
   "outputs": [],
   "source": [
    "n_words = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:56.434802Z",
     "start_time": "2023-01-18T12:43:56.373794Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lets create the actual vocab now. \n",
    "# We need one special word for 'UNKNOWN': those words that our 'out of vocabulary' for us\n",
    "# and for 'PADDING': when a sequence is less than the seuqence length we decided\n",
    "vocab = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:56.438930Z",
     "start_time": "2023-01-18T12:43:56.436720Z"
    }
   },
   "outputs": [],
   "source": [
    "n_words = n_words + 2 # for special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T09:56:50.727278Z",
     "start_time": "2023-01-18T09:56:50.687945Z"
    }
   },
   "source": [
    "!! **Good idea to go through the vocabulary, spot the fishy ones and re-adapt your preprocessing to take care of them.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Coverting tokens to word IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:58.082950Z",
     "start_time": "2023-01-18T12:43:56.440647Z"
    }
   },
   "outputs": [],
   "source": [
    "wordid_train_text = ...\n",
    "bow_train_text = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:58.314333Z",
     "start_time": "2023-01-18T12:43:58.084191Z"
    }
   },
   "outputs": [],
   "source": [
    "! free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:58.787129Z",
     "start_time": "2023-01-18T12:43:58.317968Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finally, turn them into vectors and dump to disk (keep them at float32; reshape the Y)\n",
    "\n",
    "X = ...\n",
    "Y = ...\n",
    "\n",
    "X.shape, Y.shape, X.dtype, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:59.673957Z",
     "start_time": "2023-01-18T12:43:58.788272Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dump them to disk\n",
    "\n",
    "with Path('../resources/6.1.X.np').open('wb+') as f:\n",
    "    np.save(f, X)\n",
    "    \n",
    "with Path('../resources/6.1.Y.np').open('wb+') as f:\n",
    "    np.save(f, Y)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLOSE THIS NOTEBOOK. RIP your RAM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
