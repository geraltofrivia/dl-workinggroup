{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "X -> Y makes supervised machine learning. We tried with random numbers. We tried with images. Pixels are numbers. Everything is numbers.\n",
    "\n",
    "We can treat text the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:30:52.261004Z",
     "start_time": "2023-01-18T12:30:50.663733Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:30:53.645065Z",
     "start_time": "2023-01-18T12:30:52.262473Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the dataset\n",
    "imdb = load_dataset(\"imdb\")\n",
    "\n",
    "len(imdb['train']), imdb['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Vectors\n",
    "\n",
    "![One Hot](https://miro.medium.com/max/828/1*9ZuDXoc2ek-GfHE2esty5A.webp)\n",
    "src - https://medium.com/intelligentmachines/word-embedding-and-one-hot-encoding-ad17b4bbe111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:30:53.650869Z",
     "start_time": "2023-01-18T12:30:53.647058Z"
    }
   },
   "outputs": [],
   "source": [
    "document = \"Rome Paris Italy France Potato Kartoffeln Patate\".lower()\n",
    "tokens = document.split(' ')\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:30:53.656528Z",
     "start_time": "2023-01-18T12:30:53.652700Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "for token in tokens:\n",
    "    token = token.lower()\n",
    "    if not token in vocab:\n",
    "        vocab[token] = len(vocab)\n",
    "        \n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:30:53.661726Z",
     "start_time": "2023-01-18T12:30:53.657685Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "for token in tokens:\n",
    "    vocab.setdefault(token, len(vocab))\n",
    "        \n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:30:53.668976Z",
     "start_time": "2023-01-18T12:30:53.663427Z"
    }
   },
   "outputs": [],
   "source": [
    "one_hots = []\n",
    "one_hots = np.zeros((len(vocab), len(tokens)))\n",
    "for word_nr, token in enumerate(tokens):\n",
    "    word_id = vocab[token]\n",
    "    one_hots[word_id, word_nr] = 1\n",
    "    \n",
    "one_hots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T10:18:20.080338Z",
     "start_time": "2023-01-18T10:18:20.075071Z"
    }
   },
   "source": [
    "## Multi-Hot Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:41:08.713569Z",
     "start_time": "2023-01-18T12:41:07.963074Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lets just work with 1000 documents for now\n",
    "\n",
    "train_text = \n",
    "train_labels = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess (estimate: 30-40 minutes).\n",
    "\n",
    "This is the most difficult part ^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenize Text\n",
    "\n",
    "Document is one long string of text -> One unit (pixel) can be a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:41:09.518419Z",
     "start_time": "2023-01-18T12:41:09.508124Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(document):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:41:09.831954Z",
     "start_time": "2023-01-18T12:41:09.826428Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test our basic tokenizer\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Tokenizer\n",
    "This one is actually useful.\n",
    "\n",
    "### To Install It\n",
    "\n",
    "`! pip install spacy`\n",
    "\n",
    "`! python -m spacy download en_core_web_sm`\n",
    "\n",
    "within jupyter cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:41:10.663247Z",
     "start_time": "2023-01-18T12:41:10.182161Z"
    }
   },
   "outputs": [],
   "source": [
    "# Actually useful tokenizer\n",
    "import spacy\n",
    "exclude = [\"parser\", \"tagger\", \"ner\", \"textcat\", \"attribute_ruler\", \"lemmatizer\", \"tok2vec\"]\n",
    "nlp = spacy.load(\"en_core_web_sm\", exclude=exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:41:10.677249Z",
     "start_time": "2023-01-18T12:41:10.664693Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test Spacy Tokenizer on one doc\n",
    "..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T21:55:28.747463Z",
     "start_time": "2023-01-17T21:55:25.159926Z"
    }
   },
   "source": [
    "# Tokenize Everything \n",
    "# NOTE: this might take some time. There are ways to speed it up but we dont need that for now\n",
    "tokenized_train_text = []\n",
    "for text in tqdm(train_text):\n",
    "   tokenized_train_text.append(get_spacy_tokens(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:53.826901Z",
     "start_time": "2023-01-18T12:41:10.724969Z"
    }
   },
   "outputs": [],
   "source": [
    "# This takes 2-5 minutes. We'll talk till then ^^'\n",
    "\n",
    "train_docs = ... # use NLP pipe\n",
    "tokenized_train_text = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T09:47:52.045634Z",
     "start_time": "2023-01-18T09:47:52.040144Z"
    }
   },
   "source": [
    "## 1.1 Exploring the data \n",
    "\n",
    "- Length Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T09:54:58.791484Z",
     "start_time": "2023-01-18T09:54:58.573048Z"
    }
   },
   "outputs": [],
   "source": [
    "lens = [len(doc) for doc in tokenized_train_text]\n",
    "bin_ranges = [i for i in range(0, max(lens), max(lens)//50)]\n",
    "\n",
    "#create histogram with 4 bins\n",
    "print(f\"Over {len(lens)} documents, the mean is {np.mean(lens):.2f} ± {np.std(lens):.2f}\")\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.hist(lens, bins=bin_ranges, edgecolor='black')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T09:55:22.669868Z",
     "start_time": "2023-01-18T09:55:22.664095Z"
    }
   },
   "source": [
    "# Lets decide on a maximum length of documents based on this. Say 200\n",
    "maxlen = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:55.594901Z",
     "start_time": "2023-01-18T12:43:53.828333Z"
    }
   },
   "outputs": [],
   "source": [
    "# The same setdefault stuff we did above\n",
    "vocab = {}\n",
    "...\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T22:07:52.341213Z",
     "start_time": "2023-01-17T22:07:52.334930Z"
    }
   },
   "source": [
    "### That's way too many words. 121064?\n",
    "\n",
    "Let's make sure we have only 10000 words. First 10000 words?\n",
    "NO! The most common 10000 words\n",
    "\n",
    "How?\n",
    "- count the frequency of all the tokens\n",
    "- sort it and choose top 10,000\n",
    "- turn text to IDs based on this. For the rejected words, turn them into something like 'UNKNOWN'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:55.601999Z",
     "start_time": "2023-01-18T12:43:55.596804Z"
    }
   },
   "outputs": [],
   "source": [
    "# Understanding Counters\n",
    "counter = Counter()\n",
    "\n",
    "counter.update(['the', 'red', 'pill'])\n",
    "print(counter)\n",
    "counter.update(['the', 'blue', 'gill'])\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:56.343416Z",
     "start_time": "2023-01-18T12:43:55.603783Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run a counter over our tokenized dataset\n",
    "counter = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:56.369648Z",
     "start_time": "2023-01-18T12:43:56.344954Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's see what turned out (checkout most_common)\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T10:03:57.020889Z",
     "start_time": "2023-01-18T10:03:56.409805Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the word frequencies to help decide on a vocabulary limit\n",
    "word_counts = [count for word, count in counter.most_common()]\n",
    "total_words = len(word_counts)\n",
    "\n",
    "# Create a more informative print statement\n",
    "print(f\"Vocabulary size: {total_words} unique words\")\n",
    "print(f\"Most common word appears {word_counts[0]} times\")\n",
    "print(f\"Mean word frequency: {np.mean(word_counts):.2f} ± {np.std(word_counts):.2f}\")\n",
    "\n",
    "# Create a cleaner plotting setup\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot frequency distribution on log scale (more informative for power law distributions)\n",
    "plt.subplot(211)\n",
    "plt.hist(word_counts, bins=50, color='steelblue', edgecolor='black')\n",
    "plt.title('Word Frequency Distribution (Linear Scale)')\n",
    "plt.xlabel('Word Frequency')\n",
    "plt.ylabel('Number of Words')\n",
    "\n",
    "# Plot on log scale to better visualize the long tail\n",
    "plt.subplot(212)\n",
    "plt.hist(word_counts, bins=np.logspace(0, np.log10(max(word_counts)), 50), \n",
    "         color='steelblue', edgecolor='black')\n",
    "plt.xscale('log')\n",
    "plt.title('Word Frequency Distribution (Log Scale)')\n",
    "plt.xlabel('Word Frequency (log scale)')\n",
    "plt.ylabel('Number of Words')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:56.372754Z",
     "start_time": "2023-01-18T12:43:56.370897Z"
    }
   },
   "outputs": [],
   "source": [
    "n_words = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:56.434802Z",
     "start_time": "2023-01-18T12:43:56.373794Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lets create the actual vocab now. \n",
    "# We need one special word for 'UNKNOWN': those words that our 'out of vocabulary' for us\n",
    "# and for 'PADDING': when a sequence is less than the seuqence length we decided\n",
    "vocab = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:56.438930Z",
     "start_time": "2023-01-18T12:43:56.436720Z"
    }
   },
   "outputs": [],
   "source": [
    "n_words = n_words + 2 # for special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T09:56:50.727278Z",
     "start_time": "2023-01-18T09:56:50.687945Z"
    }
   },
   "source": [
    "!! **Good idea to go through the vocabulary, spot the fishy ones and re-adapt your preprocessing to take care of them.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Coverting tokens to word IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:58.082950Z",
     "start_time": "2023-01-18T12:43:56.440647Z"
    }
   },
   "outputs": [],
   "source": [
    "wordid_train_text = ...\n",
    "bow_train_text = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:58.787129Z",
     "start_time": "2023-01-18T12:43:58.317968Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finally, turn them into vectors and dump to disk (keep them at float32; reshape the Y)\n",
    "\n",
    "X = ...\n",
    "Y = ...\n",
    "\n",
    "X.shape, Y.shape, X.dtype, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-18T12:43:59.673957Z",
     "start_time": "2023-01-18T12:43:58.788272Z"
    }
   },
   "outputs": [],
   "source": [
    "dump_dir = Path('../resources/datasets/imdb/wordid_vocab')\n",
    "dump_dir.mkdir(parents=True, exist_ok=True)\n",
    "with (dump_dir/'vocab.json').open('w+') as f:\n",
    "    json.dump(vocab, f)\n",
    "    \n",
    "with (dump_dir/'wordids_train.pkl').open('wb+') as f:\n",
    "    pickle.dump(wordid_train_text, f)\n",
    "    \n",
    "with (dump_dir/'train_labels.pkl').open('wb+') as f:\n",
    "    pickle.dump(train_labels, f)\n",
    "    \n",
    "with (dump_dir/'wordids_test.pkl').open('wb+') as f:\n",
    "    pickle.dump(wordid_test_text, f)\n",
    "    \n",
    "with (dump_dir/'test_labels.pkl').open('wb+') as f:\n",
    "    pickle.dump(test_labels, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump this to disk\n",
    "dump_dir = dump_dir.parent / 'bow_onehot'\n",
    "dump_dir.mkdir(parents=True, exist_ok=True)\n",
    "with (dump_dir / 'X_train.np').open('wb+') as f:\n",
    "    np.save(f, X)\n",
    "    \n",
    "with (dump_dir / 'Y_train.np').open('wb+') as f:\n",
    "    np.save(f, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for all the documents\n",
    "# RIP your RAM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
