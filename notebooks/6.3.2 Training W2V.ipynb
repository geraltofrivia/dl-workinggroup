{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align='right'><h3> Yes, we **used similarity** to <font color='red'>find <i>related<i> words</font> but it was designed to <font color='blue'>predict context words</font> ! <h3>  </div>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "## 1. Get a corpus\n",
    "\n",
    "Original model was trained on a 100 billion words part of Google News Corpus. \n",
    "I don't think I can find it, and we are for sure not going to be able to use it.\n",
    "\n",
    "We'll stick with Wikitext 2 version 1\n",
    "\n",
    "Links: [Wikitext 2 Description](https://paperswithcode.com/dataset/wikitext-2) [Wikitext 2 Datasets Page](https://huggingface.co/datasets/wikitext/viewer/wikitext-2-v1/train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "from tqdm.auto import tqdm, trange\n",
    "from datasets import load_dataset\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "wikitext = load_dataset(\"wikitext\", \"wikitext-2-v1\")\n",
    "wikitext, wikitext['train'][10]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals\n",
    "\n",
    "PS: we do not replicate the paper Word2Vec but do something in the same spirit for now\n",
    "\n",
    "**predict the word given its surrounding words**\n",
    "\n",
    "![Screenshot 2025-05-05 at 22.05.25.png](<attachment:Screenshot 2025-05-05 at 22.05.25.png>)\n",
    "\n",
    "\n",
    "This is what we try to model: $ P(w_i | w_{i-2}, w_{i-1}, w_{i+1} \\dots, w_{i+2})$ \n",
    "\n",
    "e.g. $P({\\text{he} | \\text{an},\\text{offer}, \\text{can't}, \\text{refuse}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets start by pre-processing the data\n",
    "\n",
    "## Get a Vocab\n",
    "\n",
    "You know the drill by now. Get a counter. Set n_words. Select top-n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a vocab\n",
    "word_counter = Counter()\n",
    "n_words = 10_000\n",
    "\n",
    "...\n",
    "    \n",
    "len(word_counter), word_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the word frequencies to make the vocab\n",
    "unk_token = '<unk>'\n",
    "vocab = {'<unk>': 0, '<pad>': 1}\n",
    "for ...\n",
    "\n",
    "print(vocab['however'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quickly convert all words to ids\n",
    "\n",
    "# Break things into words\n",
    "train_text = ...\n",
    "valid_text = ...\n",
    "\n",
    "# Just remove all docs which have no words\n",
    "train_text = [x for x in train_text if len(x)>0]\n",
    "valid_text = [x for x in valid_text if len(x)>0]\n",
    "\n",
    "# Use vocab to turn them into ids\n",
    "train_text_ids = []\n",
    "for doc in train_text:\n",
    "    train_text_ids.append(\n",
    "        [vocab.get(word, vocab['<unk>']) for word in doc]\n",
    "    )\n",
    "\n",
    "valid_text_ids = []\n",
    "for doc in valid_text:\n",
    "    valid_text_ids.append(\n",
    "        [vocab.get(word, vocab['<unk>']) for word in doc]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text[0], train_text_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = train_text[0]\n",
    "_doc = ['<pad>', '<pad>'] + doc + ['<pad>', '<pad>']\n",
    "i =4\n",
    "_i = i+2\n",
    "print(_doc)\n",
    "_doc[_i-2:_i], _doc[_i+1:_i+3], _i, _i-2, _doc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making data loaders\n",
    "\n",
    "# We want to have inputs be [w_-2, w_-1, w_+1, w_+2]. The label for this instance would be w\n",
    "\n",
    "contexts, targets = [], []\n",
    "for doc in tqdm(train_text):\n",
    "    _doc = ['<pad>', '<pad>'] + doc + ['<pad>', '<pad>']\n",
    "    print(_doc)\n",
    "    for i, word in enumerate(doc):\n",
    "        # hint _i = i +2\n",
    "        ...\n",
    "    break\n",
    "\n",
    "contexts, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale it up for the entire dataset\n",
    "pad_id = vocab['<pad>']\n",
    "\n",
    "train_contexts, train_targets = [], []\n",
    "for doc in tqdm(train_text_ids):\n",
    "    _doc = [pad_id, pad_id] + doc + [pad_id, pad_id]\n",
    "    for i, word in enumerate(doc):\n",
    "        ...\n",
    "\n",
    "valid_contexts, valid_targets = [], []\n",
    "for doc in tqdm(valid_text_ids):\n",
    "    _doc = [pad_id, pad_id] + doc + [pad_id, pad_id]\n",
    "    for i, word in enumerate(doc):\n",
    "        ...\n",
    "\n",
    "\n",
    "print(len(train_contexts), len(train_targets), len(valid_contexts), len(valid_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throw them into a dataloader\n",
    "train_contexts = ...\n",
    "train_targets = ...\n",
    "valid_contexts = ...\n",
    "valid_targets = ...\n",
    "print(train_contexts.shape, train_targets.shape)\n",
    "\n",
    "cbow_train_dataset = TensorDataset(train_contexts, train_targets)\n",
    "cbow_valid_dataset = TensorDataset(valid_contexts, valid_targets)\n",
    "\n",
    "train_dataloader = DataLoader(cbow_train_dataset, batch_size=10_000, shuffle=True)\n",
    "valid_dataloader = DataLoader(cbow_valid_dataset, batch_size=10_000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try it out:\n",
    "for batch in train_dataloader:\n",
    "    break\n",
    "\n",
    "batch[0], batch[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cbow](<../resources/cbow.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model\n",
    "\n",
    "1. We start with the four context words\n",
    "2. We assign each a vector (4 vectors, n dimensions) using an embedding layer/matrix\n",
    "3. We average four vectors to create a 'context vector'\n",
    "4. We pass the 'context vector' to the output layer\n",
    "5. We get a probability distribution over the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do it without a class now\n",
    "inputs = torch.randint(1, 10_000, (1, 4))\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make a class out of this\n",
    "class CBOW(nn.Module):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "\n",
    "\n",
    "# So let's start backproping???\n",
    "\n",
    "![computetime](https://media.tenor.com/rDKZFPwK-00AAAAC/the-matrix-keanu-reeves.gif \"backprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Too slow?\n",
    "\n",
    "Lets use a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# <font color=\"red\">Problems! </font>: Inefficient\n",
    "\n",
    "For each word pair, we compute a distribution over the enitre vocabulary.\n",
    "Why? To normalize the scores.\n",
    "\n",
    "###### Recall: \n",
    "\n",
    "**score**: $f(u.v)$ or $(u^T v)$. Our $f(.)$ was $\\text{exp}(.)$\n",
    "\n",
    "\n",
    "**normalization**: $\\sum_{i=0}^{|\\text{vocab}|} f(u.v_i)$ <- **nicht gut!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Further Reading\n",
    "\n",
    "A great overview of this entire thing - [Blogpost](https://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "\n",
    "\n",
    "Another implementation of the entire thing - [Github](https://github.com/lukysummer/SkipGram_with_NegativeSampling_Pytorch/blob/master/SkipGram_NegativeSampling.ipynb)\n",
    "\n",
    "Skip-Gram embeddings with negative embeddings is implicit factorization of the co-occurance matrix - [Paper](https://papers.nips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf)\n",
    "\n",
    "On Biases in Word Embeddings, and ways to counteract them (ony gender bias targeted in this paper) - [Paper](https://arxiv.org/pdf/1607.06520.pdf)\n",
    "\n",
    "WEAT Test - [Paper](https://arxiv.org/pdf/1608.07187.pdf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
