{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm Restart\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics\n",
    "\n",
    "- Basics of deep learning -- tensors, and gradients\n",
    "- Loss\n",
    "- Backpropagation\n",
    "- Training loop\n",
    "- Fitting a curve\n",
    "- Multi dimensional inputs\n",
    "- Limitations of polynomial models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make a tensor with a value of '3.0'. What is its datatype?\n",
    "a = torch.tensor(3)\n",
    "\n",
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.6000, dtype=torch.float64),\n",
       " tensor(1., dtype=torch.float64),\n",
       " tensor(6.5000, dtype=torch.float64),\n",
       " tensor(1.5000, dtype=torch.float64),\n",
       " tensor(10., dtype=torch.float64))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Tensor arithmetic\n",
    "a = torch.tensor(4.0)\n",
    "b = torch.tensor(2.5, dtype=torch.float64)\n",
    "\n",
    "## Adding two tensors (there are many ways)\n",
    "a+b, torch.add(a, b), a.__add__(b), torch.add(a, b) == a+b\n",
    "\n",
    "## Multiplying two tensors\n",
    "a/b, a//b, a+b, a-b, a*b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4., requires_grad=True), tensor(2.5000))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## What is the requires grad flag\n",
    "\n",
    "a = torch.tensor(4.0, requires_grad=True)\n",
    "b = torch.tensor(2.5)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.6000, grad_fn=<DivBackward0>),\n",
       " tensor(1., grad_fn=<NotImplemented>),\n",
       " tensor(6.5000, grad_fn=<AddBackward0>),\n",
       " tensor(1.5000, grad_fn=<SubBackward0>),\n",
       " tensor(10., grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Revisiting tensor operations\n",
    "a/b, a//b, a+b, a-b, a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x124894700>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a+b\n",
    "c.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we see ze graph?\n",
    "\n",
    "```bash\n",
    "$ brew install graphviz\n",
    "$ pip install torchviz\n",
    "```\n",
    "\n",
    "\n",
    "But you can just see it on my screen for now ^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Digraph.gv.pdf'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "\n",
    "a = torch.tensor(4.0, requires_grad=True)\n",
    "b = torch.tensor(2.5) # note to self: add and remove reqGrad here\n",
    "c = a / b\n",
    "d = c + b\n",
    "\n",
    "# Generate the graph\n",
    "graph = make_dot(d, params={\"a\": a, \"b\": b})\n",
    "graph.render(view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multidimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 4, 1],\n",
       "        [3, 5, 1]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Making a two dimensional tensor.\n",
    "a1d =  torch.tensor([2,4])\n",
    "a1d = torch.tensor(np.array([2, 5]))\n",
    "a2d = torch.tensor([\n",
    "    [2,4,2.],[3,5,1]\n",
    "])\n",
    "\n",
    "## What is its shape?\n",
    "a2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2],\n",
       "        [ 2, -1]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Multi-dimensional tensor addition and multiplication\n",
    "\n",
    "## 2x2 multiplications\n",
    "a = torch.tensor([[1,2], [2,-1]])\n",
    "b = torch.tensor([[1,1], [1,1]])\n",
    "\n",
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 3],\n",
       "        [1, 1]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Was this an expected result?\n",
    "\n",
    "a@b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3, 3],\n",
       "         [1, 1]]),\n",
       " tensor([[3, 3],\n",
       "         [1, 1]]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Alternatives \n",
    "\n",
    "torch.mm(a,b), torch.matmul(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor broadcasting. \n",
    "\n",
    "is a rabbit hole we are not going to go into for now. Happy to answer questions if you have some though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naked tensors, linear classificaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1167ae6d0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x_i \\in \\mathcal{X} \\subset \\mathcal{R} \\\\ y_i \\in \\mathcal{Y} \\subset \\mathcal{R}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only one data instance (make an x = 10 and y=2)\n",
    "x = torch.tensor(10.)\n",
    "y = torch.tensor(2.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  f: \\mathcal{X} \\rightarrow \\mathcal{Y} \\\\ \\hat{y}_i = f(x; (m, c)) = m\\times x + c$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4617], requires_grad=True) tensor([0.2674], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Simplest possible model -- Linear\n",
    "\n",
    "# 1. Define the parameters m and c\n",
    "m = torch.randn(1, requires_grad=True)\n",
    "c = torch.randn(1, requires_grad=True)\n",
    "\n",
    "print(m, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define the model\n",
    "def linear(x: torch.Tensor) -> torch.Tensor:\n",
    "    return (m*x) + c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ L_{mse} = \\frac{1}{2n} \\sum_{i=1}^{n} (\\hat{y_i} - y_i)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "def mse(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "    return 0.5*((y_pred-y_true)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta_i^{T+1} = \\theta_i^{T} - \\mathbf{\\gamma} \\frac{\\partial\\ L(\\ldots; \\Theta)}{\\partial\\ \\theta_i }  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a learning rate parameter\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.8839], grad_fn=<AddBackward0>) tensor(2)\n"
     ]
    }
   ],
   "source": [
    "# Compute the prediction, based on input and the model (current parameters)\n",
    "ypred = linear(x)\n",
    "\n",
    "print(ypred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Digraph.gv.pdf'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = make_dot(ypred, params={\"m\": m, \"c\": c})\n",
    "graph.render(view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.1585], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the loss based on the prediction and the true value\n",
    "\n",
    "loss = mse(ypred, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the gradient (backward call)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.1229], requires_grad=True),\n",
       " tensor([-134.1489]),\n",
       " tensor([-0.1863], requires_grad=True),\n",
       " tensor([-13.4149]),\n",
       " tensor(10),\n",
       " tensor(2))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the gradient values. What patterns do we see?\n",
    "m, m.grad, c, c.grad, x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Digraph.gv.pdf'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = make_dot(loss, params={\"m\": m, \"c\": c})\n",
    "graph.render(view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All together now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor([89.9797], grad_fn=<MulBackward0>)\n",
      "True: 2\n",
      "Pred: -11.414891242980957\n",
      "Parameters before update:\n",
      "\tm: -1.1228563785552979\tgrad: -134.14891052246094\n",
      "\tc: -0.18632829189300537\tgrad: -13.414891242980957\n",
      "Parametrs after update:\n",
      "\tm: 0.21863269805908203\tgrad: -134.14891052246094\n",
      "\tc: -0.052179381251335144\tgrad: -13.414891242980957\n",
      "------ 0 ------\n",
      "Loss: tensor([0.0090], grad_fn=<MulBackward0>)\n",
      "True: 2\n",
      "Pred: 2.1341476440429688\n",
      "Parameters before update:\n",
      "\tm: 0.21863269805908203\tgrad: 1.3414764404296875\n",
      "\tc: -0.052179381251335144\tgrad: 0.13414764404296875\n",
      "Parametrs after update:\n",
      "\tm: 0.20521792769432068\tgrad: 1.3414764404296875\n",
      "\tc: -0.05352085828781128\tgrad: 0.13414764404296875\n",
      "------ 1 ------\n",
      "Loss: tensor([8.9992e-07], grad_fn=<MulBackward0>)\n",
      "True: 2\n",
      "Pred: 1.9986584186553955\n",
      "Parameters before update:\n",
      "\tm: 0.20521792769432068\tgrad: -0.013415813446044922\n",
      "\tc: -0.05352085828781128\tgrad: -0.0013415813446044922\n",
      "Parametrs after update:\n",
      "\tm: 0.2053520828485489\tgrad: -0.013415813446044922\n",
      "\tc: -0.05350744351744652\tgrad: -0.0013415813446044922\n",
      "------ 2 ------\n",
      "Loss: tensor([9.2342e-11], grad_fn=<MulBackward0>)\n",
      "True: 2\n",
      "Pred: 2.000013589859009\n",
      "Parameters before update:\n",
      "\tm: 0.2053520828485489\tgrad: 0.00013589859008789062\n",
      "\tc: -0.05350744351744652\tgrad: 1.3589859008789062e-05\n",
      "Parametrs after update:\n",
      "\tm: 0.20535072684288025\tgrad: 0.00013589859008789062\n",
      "\tc: -0.05350757762789726\tgrad: 1.3589859008789062e-05\n",
      "------ 3 ------\n",
      "Loss: tensor([2.8422e-14], grad_fn=<MulBackward0>)\n",
      "True: 2\n",
      "Pred: 1.999999761581421\n",
      "Parameters before update:\n",
      "\tm: 0.20535072684288025\tgrad: -2.384185791015625e-06\n",
      "\tc: -0.05350757762789726\tgrad: -2.384185791015625e-07\n",
      "Parametrs after update:\n",
      "\tm: 0.20535075664520264\tgrad: -2.384185791015625e-06\n",
      "\tc: -0.053507573902606964\tgrad: -2.384185791015625e-07\n",
      "------ 4 ------\n",
      "Loss: tensor([0.], grad_fn=<MulBackward0>)\n",
      "True: 2\n",
      "Pred: 2.0\n",
      "Parameters before update:\n",
      "\tm: 0.20535075664520264\tgrad: 0.0\n",
      "\tc: -0.053507573902606964\tgrad: 0.0\n",
      "Parametrs after update:\n",
      "\tm: 0.20535075664520264\tgrad: None\n",
      "\tc: -0.053507573902606964\tgrad: None\n",
      "------ 5 ------\n",
      "Model fully converged. Stopping.\n"
     ]
    }
   ],
   "source": [
    "values_of_loss = []\n",
    "values_of_m = []\n",
    "values_of_c = []\n",
    "values_of_ypred = []\n",
    "\n",
    "for i in range(2000):\n",
    "    # Calcualte model predictions\n",
    "    y_pred = linear(x)\n",
    "    \n",
    "    \n",
    "    # Compare the prediction with our goal\n",
    "    loss = mse(y_pred, y)\n",
    "    print(f\"Loss: {loss}\\nTrue: {y}\\nPred: {y_pred.item()}\")\n",
    "    \n",
    "    # Reset the gradients before computing new ones\n",
    "    if m.grad:\n",
    "        # PS: tensor.grad.zero_() is an inplace operation\n",
    "        # PPS: You will never do this again in your life.\n",
    "        m.grad.zero_()\n",
    "        c.grad.zero_()\n",
    "        \n",
    "    # Compute new gradients: BACKPROPAGATE\n",
    "    loss.backward()\n",
    "    \n",
    "    print(f\"Parameters before update:\\n\\tm: {m.item()}\\tgrad: {m.grad.item()}\\n\\tc: {c.item()}\\tgrad: {c.grad.item()}\")\n",
    "    with torch.no_grad():\n",
    "        # Do the actual update\n",
    "        updated_m = m - (lr*m.grad)\n",
    "        updated_c = c - (lr*c.grad)\n",
    "        m.copy_(updated_m)\n",
    "        c.copy_(updated_c)\n",
    "        \n",
    "    print(f\"Parametrs after update:\\n\\tm: {m.item()}\\tgrad: {m.grad.item()  if c.grad else None}\\n\\tc: {c.item()}\\tgrad: {c.grad.item() if c.grad else None}\")\n",
    "\n",
    "    # Bookkeeping \n",
    "    values_of_ypred.append(y_pred.item())\n",
    "    values_of_m.append(m.item())\n",
    "    values_of_c.append(c.item())\n",
    "    values_of_loss.append(loss.item())\n",
    "\n",
    "    print('------', i, '------')\n",
    "    cmd = input().strip()\n",
    "    if cmd in ['q', 'exit', 'break']:\n",
    "        break\n",
    "    \n",
    "    if loss.item() == 0:\n",
    "        print('Model fully converged. Stopping.')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Structure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping what we did in a class\n",
    "\n",
    "class LinearRegressor(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.m = torch.nn.Parameter(torch.tensor(1.))\n",
    "        self.c = torch.nn.Parameter(torch.tensor(1.))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (self.m*x) + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model class\n",
    "class LinearRegressor(torch.nn.Module):\n",
    "\n",
    "    def __init__(self: \"LinearRegressor\"):\n",
    "        super().__init__()\n",
    "        self.layer_1 = torch.nn.Linear(1, 1)\n",
    "    \n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layer_1(inputs)\n",
    "    \n",
    "model = LinearRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss should never be defined by us\n",
    "mse = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': [Parameter containing:\n",
       "   tensor([[0.7826]], requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([-0.7105], requires_grad=True)],\n",
       "  'lr': 0.001,\n",
       "  'momentum': 0,\n",
       "  'dampening': 0,\n",
       "  'weight_decay': 0,\n",
       "  'nesterov': False,\n",
       "  'maximize': False,\n",
       "  'foreach': None,\n",
       "  'differentiable': False,\n",
       "  'fused': None}]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Optimizer\n",
    "opt = torch.optim.SGD(model.parameters())\n",
    "\n",
    "opt.param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.7826]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.7105], requires_grad=True)]\n",
      "[Parameter containing:\n",
      "tensor([[0.7842]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.7081], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "## Playing around with optimizer and parameters\n",
    "x = torch.randn(1,1)\n",
    "y = torch.tensor([1.])\n",
    "\n",
    "## lets emulate a 'batch'\n",
    "ypred = model(x)\n",
    "loss = mse(ypred, y)\n",
    "loss.backward()\n",
    "print(list(model.parameters()))\n",
    "\n",
    "opt.step()\n",
    "\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets sketch out a nice loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's all for now\n",
    "\n",
    "Next time, we're gonna try some real world problems, and focus on data handling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
