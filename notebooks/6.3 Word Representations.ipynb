{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TjTU0Z_waFz"
   },
   "source": [
    "# This notebook has been adapted from \n",
    "\n",
    "Machine Learning for NLP - ENSAE 2022 - Lecture 2 illustration \n",
    "\n",
    "This notebook aims at illustrating the concepts introduced in the [lecture 2](https://nlp-ensae.github.io/files/2-ML-FOR-NLP-2022.pdf) of the Machine Learning for NLP course\n",
    "\n",
    "Slides from https://nlp-ensae.github.io/files/2-ML-FOR-NLP-2022.pdf\n",
    "(Skip to slide 17)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T08:48:28.217695Z",
     "start_time": "2023-02-09T08:48:28.212474Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T08:46:41.870687Z",
     "start_time": "2023-02-09T08:46:41.863976Z"
    }
   },
   "source": [
    "# One Hot Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T08:50:28.161223Z",
     "start_time": "2023-02-09T08:50:28.150027Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab  = {'potato', 'petrolium', 'carrots', 'garlic', 'mac', 'basil', 'piano', 'flour'}\n",
    "vecs = np.zeros((len(vocab), len(vocab)))\n",
    "vecs[[range(len(vocab))], [range(len(vocab))]] = 1\n",
    "\n",
    "potato, petrolium, carrots, garlic, mac, basil, piano, flour = vecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T08:52:01.099503Z",
     "start_time": "2023-02-09T08:52:01.094328Z"
    }
   },
   "outputs": [],
   "source": [
    "# What is the similarity between petrolium and potato?\n",
    "# What about carrots and potato?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity between vectors ðŸ¤”\n",
    "\n",
    "## Dot Product?\n",
    "\n",
    "$$\n",
    "\\textbf{a} = [a_1, a_2, \\dots , a_n]\\\\\\textbf{b} = [b_1, b_2, \\dots , b_n] \\\\ \\textbf{a}.\\textbf{b} = \\sum^{n}_{i=1}a_ib_i\n",
    "$$\n",
    "\n",
    "- Vectors with a larger magnitude will have higher 'score'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine?\n",
    "\n",
    "$$\n",
    "\\text{cos}(\\textbf{a},\\textbf{b}) = \\frac{\\textbf{a} . \\textbf{b}}{||\\textbf{a}|| . ||\\textbf{b}||}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T09:07:34.807351Z",
     "start_time": "2023-02-09T09:07:34.785729Z"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine(v1, v2):\n",
    "  \"\"\"\n",
    "  cosine similarity of two vectors\n",
    "  NB: Standard metric to measure similarity between word vectors\n",
    "  \n",
    "  Hint: look at the imports in cell above ;)\n",
    "  \"\"\"\n",
    "  ...\n",
    "\n",
    "a,b = np.array([1,1,1,1]), np.array([-1,2,-1,2])\n",
    "print(f\"Cosine of {a}, {b} = {cosine(a,b)}\")\n",
    "\n",
    "a,b = np.array([1,1,1,1]), np.array([-2,-2,-2,-2])\n",
    "print(f\"Cosine of {a}, {b} = {cosine(a,b)}\")\n",
    "\n",
    "a,b = np.array([1,1,1,1]), np.array([20,20,20,20])\n",
    "print(f\"Cosine of {a}, {b} = {cosine(a,b)}\")\n",
    "\n",
    "a,b = np.array([1,1,1,1]), np.array([-9,20,-7,-4])\n",
    "print(f\"Cosine of {a}, {b} = {cosine(a,b)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJBKyC-e9Awv"
   },
   "source": [
    "# Count-Based Representations\n",
    "\n",
    "Same dimensions as one-hot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T09:08:22.269492Z",
     "start_time": "2023-02-09T09:08:22.264303Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Co-Occurences of dog, lion and car \n",
    "#  (leash,walk, run, owner, pet, barked, the)\n",
    "dog =  [3,   5,   2,     5,   3,      2,  9]\n",
    "lion = [0,   3,   2,     0,   1,      0,  5]\n",
    "car =  [0,   0,   1,     3,   0,      0,  9]\n",
    "\n",
    "# dog = np.array(dog)\n",
    "# lion = np.array(lion)\n",
    "# car =  np.array(car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T09:08:25.327810Z",
     "start_time": "2023-02-09T09:08:25.316221Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(f'Example 1: Original similarities \\nCosine Similarity dog vs. lion {cosine(dog, lion):.4f} ')\n",
    "print(f'Cosine Similarity dog vs. car {cosine(dog, car):.4f} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T09:37:53.694479Z",
     "start_time": "2023-02-09T09:37:53.679842Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now assume 'the dog' and 'the car' appear much more \n",
    "# (leash, walk, run, owner, pet, barked, the)\n",
    "dog =  [3,   5,   2,     5,   3,      2,  9+25]\n",
    "car =  [0,   0,   1,     3,   0,      0,  9+25]\n",
    "\n",
    "print(f'Example 2: Now assume \"the dog\" and \"the car\" appear much more  \\nCosine Similarity dog vs. lion {cosine(dog, lion):.4f}')\n",
    "print(f'Cosine Similarity dog vs. car {cosine(dog, car):.4f} ')\n",
    "print(f'What do you observe? \\n')\n",
    "# Frequent words impacts a lot the representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T09:08:46.055506Z",
     "start_time": "2023-02-09T09:08:46.042056Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now assume 'the dog' and 'the car' appear much more \n",
    "# (leash,   walk, run, owner, pet, barked, the)\n",
    "dog =  [3+1,   5,   2,     5,   3,      2,   9]\n",
    "car =  [0+1,   0,   1,     3,   0,      0,   9]\n",
    "\n",
    "print(f'Example 3: Now assume \"dog leash\" and \"car leash\" occur.  \\nCosine Similarity dog vs. lion {cosine(dog, lion):.4f}')\n",
    "print(f'Cosine Similarity dog vs. car {cosine(dog, car):.4f} ')\n",
    "print(f'What do you observe? \\n')\n",
    "# Very high sensitivity to rare words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "- Count based similarity is super sensitive to less informational, very frequent word (the)\n",
    "- Count based similarity is super sensitive to rare words as well (leash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T17:14:26.183946Z",
     "start_time": "2023-02-08T17:14:26.174756Z"
    }
   },
   "source": [
    "## Alternatives?\n",
    "\n",
    "TF-IDF solves this problem. \n",
    "\n",
    "[Good Article explaining it](https://towardsdatascience.com/tf-idf-simplified-aba19d5f5530) | \n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "\n",
    "Q: **why not use tf-idf then?**\n",
    "\n",
    "A: Its not a way to represent tokens, only a way to compute a score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pointwise Mutual Information\n",
    "\n",
    "**Estimate association between events** (events = tokens, for us).\n",
    "- `PMI(\"barack\", \"obama\") =   20.0`: Does \"Barack\" often appear with \"Obama\" (yes â†’ high positive value)? **they're correlated**\n",
    "- `PMI(\"barack\", \"pizza\") =    0.3`: Or does it appear together with \"Peacock\" (independent â†’ close to zero)? **they're not correlated**\n",
    "- `PMI(\"barack\", \"racist\") = -13.8`: Or does it appear together with \"racist\"?\n",
    "    - In fact no, it appears less frequently with \"Barack\". I.e., **they're inversely correlated**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [PMI] is one of the most important concepts in NLP - Ch 6, Speech and Language Processing, Jurafsky and Martin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###### Let's work with this co-occurance matrix\n",
    "\n",
    "![coccurance.png](./../resources/imgs/word-wordcooccurancematrix.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T09:46:52.128769Z",
     "start_time": "2023-02-09T09:46:52.107789Z"
    }
   },
   "outputs": [],
   "source": [
    "cooccurance = np.asarray([\n",
    "    [3,  5,   2,   5,     3,   2 ,    8],\n",
    "    [0,  3,   2,   0,     1,   0,     6],\n",
    "    [0,  0,   1,   3,     0,   0,     3]\n",
    "])\n",
    "\n",
    "# (leash, walk, run, owner, pet, barked, the)\n",
    "dog =  np.array([3,  5,   2,   5,     3,   2 ,    8])\n",
    "lion = np.array([0,  3,   2,   0,     1,   0,     6])\n",
    "car =  np.array([0,  0,   1,   3,     0,   0,     3])\n",
    "\n",
    "leash =  np.array([3, 0, 0])\n",
    "walk =   np.array([5, 3, 0])\n",
    "run =    np.array([2, 2, 1])\n",
    "owner =  np.array([5, 0, 3])\n",
    "pet =    np.array([3, 1, 0])\n",
    "barked = np.array([2, 0, 0])\n",
    "the =    np.array([8, 6, 3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PMI Formula\n",
    "    \n",
    "$$\\text{pmi}(x, y) = \\text{ln} \\big( \\frac{p(x, y)}{p(x) p(y)}\\big)$$\n",
    "\n",
    "Its a simple co-relation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T10:14:45.253158Z",
     "start_time": "2023-02-09T10:14:45.246386Z"
    }
   },
   "outputs": [],
   "source": [
    "def ppmi(mat, ix, iy):\n",
    "    # avoid taking log of zeros; return a zero instead\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T10:21:45.749148Z",
     "start_time": "2023-02-09T10:21:45.744169Z"
    }
   },
   "outputs": [],
   "source": [
    "print(ppmi(cooccurance, 1, 0)) # dog, leash\n",
    "print(ppmi(cooccurance, 0, 1)) # dog, walk\n",
    "print(ppmi(cooccurance, 0, 2)) # dog, run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PMIVEC.jpg](./../resources/imgs/pmivec.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T10:14:50.620135Z",
     "start_time": "2023-02-09T10:14:50.615327Z"
    }
   },
   "outputs": [],
   "source": [
    "def pmivec(cooccurance, ix):\n",
    "    return np.array([ppmi(cooccurance, ix, iy) for iy in range(cooccurance.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T10:22:20.682965Z",
     "start_time": "2023-02-09T10:22:20.667684Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7UQTwR-LODEf",
    "outputId": "bfe4da9f-9a93-4c50-e4a1-3d5b2c8f65cc"
   },
   "outputs": [],
   "source": [
    "dog_pmi = pmivec(cooccurance, 0)\n",
    "lion_pmi = pmivec(cooccurance, 1)\n",
    "car_pmi = pmivec(cooccurance, 2)\n",
    "\n",
    "print(dog_pmi)\n",
    "print(lion_pmi)\n",
    "print(car_pmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T19:03:09.478999Z",
     "start_time": "2023-02-08T19:03:09.469226Z"
    }
   },
   "source": [
    "# Problems with all of these methods?\n",
    "\n",
    "## Dimensionality\n",
    "\n",
    "![NwordsStuff.jpg](./../resources/imgs/nwords_per_doc.png)\n",
    "\n",
    "x axis - number of documents, y axis - number of unique words\n",
    "\n",
    "With more documents, come more words. Every token will be 33k dimensions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJKDdZ96P54f"
   },
   "source": [
    "# Prediction-Based Representation: Word2vec - Skip Gram\n",
    "\n",
    "Dense vectors: every token exists in a n-dimensional space. \n",
    "- Different dimensions of the space encode different latent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T10:43:42.749825Z",
     "start_time": "2023-02-09T10:43:42.632472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:            15G        5,6G        7,9G        689M        1,8G        8,7G\r\n",
      "Swap:          979M        978M        1,9M\r\n"
     ]
    }
   ],
   "source": [
    "! free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets manually read the vectors\n",
    "vocab, vectors = {}, []\n",
    "n_words = 20000\n",
    "with Path('../resources/vectors/wikinews/wiki-news-300d-1M.vec').open('r') as f:\n",
    "    for i, line in tqdm(enumerate(f.readlines())):\n",
    "        if i > 0:\n",
    "            vocab.setdefault(line.split()[0].strip(), len(vocab))\n",
    "            vectors.append([float(x.strip()) for x in line.split()[1:]])\n",
    "        if i > n_words:\n",
    "            break\n",
    "            \n",
    "vectors = np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T10:39:47.850409Z",
     "start_time": "2023-02-09T10:39:47.569046Z"
    }
   },
   "outputs": [],
   "source": [
    "class WordVectors:\n",
    "    \n",
    "    def __init__(self, vocab, vectors):\n",
    "        self.vocab = vocab\n",
    "        self.vectors = vectors if isinstance(vectors, np.ndarray) else np.array(vectors)\n",
    "        self.zerovec = np.zeros((vectors.shape[1]), dtype=np.float)\n",
    "        \n",
    "        self._norm = norm(vectors, axis=1)\n",
    "        self.id_to_tok = {i:tok for tok, i in vocab.items()}\n",
    "        \n",
    "    def __call__(self, k):\n",
    "        try:\n",
    "            ind = self.vocab[k]\n",
    "            return self.vectors[ind]\n",
    "        except KeyError:\n",
    "            return self.zerovec\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def cosine(self, va, vb):\n",
    "        if isinstance(va, str):\n",
    "            va = self.vocab.get(va, -1)\n",
    "        if isinstance(vb, str):\n",
    "            vb = self.vocab.get(vb, -1)\n",
    "    \n",
    "    def mo(self, word, k: 10):\n",
    "        wid = self.vocab.get(word, -1)\n",
    "        if wid < 0:\n",
    "            return None\n",
    "        v = self.vectors[wid]\n",
    "        cosines = np.dot(self.vectors, v) / self._norm / norm(v)\n",
    "        cosines_ind = np.argsort(-cosines)[:k]\n",
    "        return [(self.id_to_tok[tokid], cosines[tokid]) for tokid in cosines_ind]\n",
    "        \n",
    "\n",
    "w2v = WordVectors(vocab, np.array(vectors))\n",
    "len(vectors), len(vocab), w2v.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T10:39:48.543199Z",
     "start_time": "2023-02-09T10:39:48.537218Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"W2V Cosine of 'dog', and 'cat' is {cosine(w2v('dog'), w2v('cat')):.4f}\")\n",
    "print(f\"W2V Cosine of 'dog', and 'fork' is {cosine(w2v('dog'), w2v('fork')):.4f}\")\n",
    "print('----')\n",
    "print(f\"W2V Cosine of 'pen', and 'man' is {cosine(w2v('pen'), w2v('man')):.4f}\")\n",
    "print(f\"W2V Cosine of 'pen', and 'pencil' is {cosine(w2v('pen'), w2v('pencil')):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T10:40:14.262818Z",
     "start_time": "2023-02-09T10:40:14.244208Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e1smZSmUPU_s",
    "outputId": "9fc733a1-95d8-438d-b9eb-c949a2a73d1c"
   },
   "outputs": [],
   "source": [
    "# Similarity \n",
    "# 1- computing the cosine between the vector of 'dog' and all the other words\n",
    "# 2- get the top-10 most similar words \n",
    "# model.wv.most_similar does those two steps \n",
    "w2v.mo('dog', 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T10:41:22.825780Z",
     "start_time": "2023-02-09T10:41:22.819886Z"
    },
    "id": "61mrlYXOPg2c"
   },
   "outputs": [],
   "source": [
    "# Visualization\n",
    "from sklearn.decomposition import PCA\n",
    "#%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.style.use('ggplot')\n",
    "\n",
    "def plot_word2vec_embedding(model, projection=\"pca\", \n",
    "                            words=None, \n",
    "                            plot_dir=None,\n",
    "                            sample=0):\n",
    "        \n",
    "    vecs = np.array([model(w) for w in words])\n",
    "\n",
    "    if projection == \"pca\":\n",
    "      pca_representations = PCA().fit_transform(vecs)[:,:2]\n",
    "    else:\n",
    "      raise(Exception(f\"{projection} not supported\"))\n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(pca_representations[:,0], pca_representations[:,1], edgecolors='k', c='r')\n",
    "    for word, (x,y) in zip(words, pca_representations):\n",
    "        plt.text(x+0.05, y+0.05, word)\n",
    "    if plot_dir:\n",
    "      plt.savefig(plot_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T10:41:23.451047Z",
     "start_time": "2023-02-09T10:41:23.095564Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "_9vy93K5Qc2Y",
    "outputId": "54abfec3-7b20-4c96-f5c2-7ba2b9077b23"
   },
   "outputs": [],
   "source": [
    "plot_word2vec_embedding(w2v, \n",
    "                        words = ['dog', 'cat', 'animal', 'elephant', 'car', 'human','horse', 'monkey', 'parrot', 'koala', 'lizard',\n",
    "                         #  'eating', 'barking',\n",
    "                         'man', 'woman',\"girl\",\"boy\",\"uncle\", \n",
    "                         \"he\", \"she\", \"her\", \"him\",\n",
    "                         \"the\", \"that\",\"this\",\n",
    "                         'paris', 'madrid', 'london', \"rome\"],\n",
    "                         plot_dir=\"./plot_2.png\",\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources for in-depth reads\n",
    "\n",
    "- Ch 6, Speech and Language Processing - https://web.stanford.edu/~jurafsky/slp3/6.pdf (great book; great chapter)\n",
    "- Great article on understanding the intuition of PMI - https://svn.spraakdata.gu.se/repos/gerlof/pub/www/Docs/npmi-pfd.pdf"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
